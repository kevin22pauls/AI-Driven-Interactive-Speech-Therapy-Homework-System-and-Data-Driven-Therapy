================================================================================
           AI-POWERED SPEECH THERAPY HOMEWORK SYSTEM
            COMPREHENSIVE PROJECT DOCUMENTATION
================================================================================

PROJECT OVERVIEW
================================================================================
This is a research-grade speech therapy system designed for patients with
aphasia and speech disorders. The system provides automated speech analysis
at multiple levels: word-level, phoneme-level, and semantic-level.

Intended Use: Clinical research and speech therapy assessment
Target Population: Aphasia patients, dysarthria, and speech sound disorders
Research Status: Suitable for journal publication


SYSTEM ARCHITECTURE
================================================================================

┌─────────────────────────────────────────────────────────────────────────┐
│                           FRONTEND LAYER                                 │
│  ┌──────────────────────────────────────────────────────────────────┐  │
│  │  recorder.html - Browser-based Recording Interface               │  │
│  │  - MediaRecorder API for audio capture                           │  │
│  │  - Real-time silence detection (Web Audio API)                   │  │
│  │  - Result visualization with phoneme error highlighting          │  │
│  └──────────────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────────────┘
                                    ↓ HTTP POST /record
┌─────────────────────────────────────────────────────────────────────────┐
│                         BACKEND API LAYER                                │
│  ┌──────────────────────────────────────────────────────────────────┐  │
│  │  main.py - FastAPI Application                                   │  │
│  │  routers/analysis.py - API Endpoints                             │  │
│  │    - /session/start         - Initialize session                 │  │
│  │    - /session/next-prompt   - Get next prompt                    │  │
│  │    - /session/change-object - Switch object                      │  │
│  │    - /record                - Upload & analyze audio             │  │
│  └──────────────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────────────┘
                                    ↓
┌─────────────────────────────────────────────────────────────────────────┐
│                        ANALYSIS PIPELINE                                 │
│  ┌──────────────────────────────────────────────────────────────────┐  │
│  │  services/speech_processing.py - Main Pipeline                   │  │
│  └──────────────────────────────────────────────────────────────────┘  │
│                                                                          │
│  ┌─────────────┬─────────────────┬───────────────────┬──────────────┐  │
│  │  ASR        │  Word-Level     │  Phoneme-Level    │  Semantic    │  │
│  │  (Whisper)  │  Metrics        │  Analysis         │  Evaluation  │  │
│  └─────────────┴─────────────────┴───────────────────┴──────────────┘  │
│        ↓              ↓                   ↓                  ↓          │
│  Transcription   WER, Rate,         PER, Error          Similarity,    │
│  + Timestamps    Pause Ratio        Classification      Classification │
└─────────────────────────────────────────────────────────────────────────┘
                                    ↓
┌─────────────────────────────────────────────────────────────────────────┐
│                        DATA PERSISTENCE                                  │
│  ┌──────────────────────────────────────────────────────────────────┐  │
│  │  SQLite Database (speechtherapy.db)                              │  │
│  │  - recordings table: All analysis results                        │  │
│  │  - patient_phoneme_history: Longitudinal phoneme tracking        │  │
│  └──────────────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────────────┘


SUCCESSFULLY IMPLEMENTED FEATURES
================================================================================

1. AUTOMATIC SPEECH RECOGNITION (ASR)
   ✓ Faster-Whisper model integration (small/tiny models)
   ✓ CPU-optimized inference (int8 quantization)
   ✓ Segment-level timestamps
   ✓ Multi-format audio support (webm, wav, mp3)
   ✓ Robust error handling for transcription failures

2. WORD-LEVEL METRICS
   ✓ Word Error Rate (WER) calculation using jiwer
   ✓ Speech rate (words per minute)
   ✓ Pause ratio (silence percentage)
   ✓ Segment-level analysis

3. PHONEME-LEVEL ANALYSIS ⭐ NEW
   ✓ CMU Pronouncing Dictionary integration (134,000+ words)
   ✓ Phoneme Error Rate (PER) calculation
   ✓ Error type classification:
      - Substitutions (e.g., AA → UW in "bottle" → "bootle")
      - Deletions (missing phonemes)
      - Insertions (extra phonemes)
   ✓ Problematic phoneme identification
   ✓ Clinical insights generation
   ✓ Forced alignment support (Gentle integration)
   ✓ ARPAbet to IPA conversion for display
   ✓ Database storage for longitudinal tracking

4. SEMANTIC ANALYSIS
   ✓ Sentence transformer-based similarity (all-MiniLM-L6-v2)
   ✓ Three-tier classification (correct/partial/wrong)
   ✓ Object identification verification
   ✓ Paraphrase detection
   ✓ Semantic substitution tracking
   ✓ Clinical marker identification

5. PROMPT SYSTEM
   ✓ Object-based prompt generation
   ✓ 5 objects with multiple prompts each:
      - bottle, cup, phone, book, chair
   ✓ Question types: identification, functional, descriptive
   ✓ Expected answer validation
   ✓ Session-based prompt management
   ✓ Multi-prompt sessions without changing objects
   ✓ Dynamic object switching

6. USER INTERFACE
   ✓ Browser-based audio recording
   ✓ Automatic silence detection (VAD)
   ✓ Real-time status updates
   ✓ Comprehensive result display:
      - Transcript
      - Semantic evaluation with color coding
      - Phoneme error table with highlighting
      - Speech metrics
      - Clinical insights
   ✓ Collapsible detailed views
   ✓ Session management buttons

7. DATABASE & STORAGE
   ✓ SQLite database with comprehensive schema
   ✓ Recordings table with all metrics
   ✓ Patient phoneme history tracking
   ✓ JSON storage for complex data
   ✓ Indexed for performance
   ✓ Audio file management with timestamped names

8. TESTING & QUALITY ASSURANCE
   ✓ Unit tests for phoneme analysis
   ✓ Test coverage for PER calculation
   ✓ Alignment algorithm testing
   ✓ Clinical note generation tests
   ✓ CMU dictionary lookup tests

9. DEPLOYMENT & INFRASTRUCTURE
   ✓ Docker Compose configuration for Gentle
   ✓ Modular service architecture
   ✓ Environment variable configuration
   ✓ Graceful degradation (works without Gentle)
   ✓ Comprehensive logging


TECHNICAL SPECIFICATIONS
================================================================================

BACKEND STACK:
  - Framework: FastAPI 0.x
  - ASGI Server: Uvicorn
  - Database: SQLite with SQLAlchemy ORM
  - ML Models:
      * Faster-Whisper (small/tiny) for ASR
      * Sentence-Transformers (all-MiniLM-L6-v2) for semantics
  - Phoneme Analysis:
      * CMU Pronouncing Dictionary (134,000+ words)
      * Edit distance algorithms for alignment
      * Gentle forced aligner (optional, via Docker)

FRONTEND STACK:
  - Pure HTML5 + Vanilla JavaScript (no frameworks)
  - Web APIs:
      * MediaRecorder API (audio capture)
      * Web Audio API (silence detection)
      * Fetch API (HTTP communication)

AUDIO PROCESSING:
  - Supported formats: webm, wav, mp3, ogg, flac
  - Silence detection: RMS-based energy threshold
  - Auto-stop: 900ms silence or 8s timeout

DEPENDENCIES (requirements.txt):
  - fastapi
  - uvicorn
  - sqlalchemy
  - python-multipart
  - pydantic
  - faster-whisper
  - torch
  - numpy
  - scipy
  - noisereduce
  - silero-vad
  - jiwer
  - sentence-transformers
  - soundfile
  - editdistance
  - requests
  - scikit-learn


FILE STRUCTURE
================================================================================

FINAL PROJECT/
├── backend/
│   ├── main.py                      # FastAPI application entry point
│   ├── config.py                    # Configuration settings
│   ├── requirements.txt             # Python dependencies
│   │
│   ├── routers/
│   │   └── analysis.py              # API endpoints
│   │
│   ├── services/
│   │   ├── speech_processing.py     # Main analysis pipeline
│   │   ├── semantic_analysis.py     # Semantic evaluation
│   │   ├── phoneme_analysis.py      # ⭐ Phoneme-level analysis
│   │   ├── phoneme_lookup.py        # ⭐ CMU Dict wrapper
│   │   ├── forced_alignment.py      # ⭐ Gentle integration
│   │   ├── prompts.py               # Prompt management
│   │   ├── metrics.py               # WER, rate, pause calculations
│   │   └── audio.py                 # Audio preprocessing
│   │
│   ├── models/
│   │   └── whisper_model.py         # Whisper ASR singleton
│   │
│   ├── database/
│   │   ├── db.py                    # SQLAlchemy engine
│   │   ├── schema.py                # Table initialization
│   │   └── schema.sql               # ⭐ Updated SQL schema
│   │
│   ├── data/
│   │   ├── objects.py               # Object definitions
│   │   └── cmudict-0.7b.txt         # ⭐ CMU Pronouncing Dictionary
│   │
│   ├── utils/
│   │   └── file_utils.py            # File management
│   │
│   ├── frontend/
│   │   └── recorder.html            # ⭐ Updated UI with phoneme display
│   │
│   ├── storage/
│   │   └── audio/                   # Audio recordings (gitignored)
│   │
│   └── tests/                       # ⭐ NEW
│       ├── __init__.py
│       └── test_phoneme_analysis.py # Unit tests
│
├── docker-compose.yml               # ⭐ Gentle service configuration
├── .gitignore                       # Git ignore rules
└── PROJECT_DOCUMENTATION.txt        # ⭐ This file


KEY ALGORITHMS
================================================================================

1. PHONEME ERROR RATE (PER) CALCULATION:

   PER = (S + D + I) / N

   Where:
   - S = Number of phoneme substitutions
   - D = Number of phoneme deletions
   - I = Number of phoneme insertions
   - N = Total number of phonemes in reference

   Implementation: Levenshtein distance on phoneme sequences

2. PHONEME ALIGNMENT:

   Uses dynamic programming (similar to sequence alignment in bioinformatics)
   to find optimal alignment between expected and actual phoneme sequences.

   Algorithm: Wagner-Fischer algorithm (edit distance with backtracking)

   Example:
   Expected: B  AH  T  AH  L    (bottle)
   Actual:   B  UW  T  AH  L    (bootle)
   Error:       ^                (substitution: AH → UW)

3. SEMANTIC SIMILARITY:

   Uses sentence transformers to compute cosine similarity between
   expected answer and transcript embeddings.

   Thresholds:
   - > 0.85: Correct
   - 0.60 - 0.85: Partial
   - < 0.60: Wrong

4. SPEECH RATE CALCULATION:

   Speech Rate = Total Words / (End Time - Start Time) * 60

   Units: Words per minute (WPM)

5. PAUSE RATIO:

   Pause Ratio = Total Pause Duration / Total Speech Duration

   Calculated from inter-segment gaps


CLINICAL INSIGHTS GENERATION
================================================================================

The system automatically generates clinical recommendations based on:

1. Overall PER score
2. Error type distribution
3. Specific problematic phonemes
4. Error patterns (e.g., consistent substitutions)
5. Phoneme class (vowels, fricatives, liquids)

Example Output:
"Moderate phoneme errors detected"
"Most problematic phonemes: AA (2x), AH (1x)"
"Common substitution pattern: AA→UW"
"Consider vowel discrimination exercises"


RESEARCH APPLICATIONS
================================================================================

This system enables research on:

1. Phoneme-level error patterns in aphasia
2. Longitudinal tracking of pronunciation improvement
3. Identification of problematic phonemes per patient
4. Correlation between semantic and phonetic errors
5. Automated homework compliance monitoring
6. Large-scale speech therapy outcome studies


USAGE INSTRUCTIONS
================================================================================

SETUP:

1. Install Python dependencies:
   cd backend
   pip install -r requirements.txt

2. (Optional) Start Gentle service for forced alignment:
   docker-compose up -d gentle

3. Start the backend server:
   cd backend
   uvicorn main:app --reload --port 8000

4. Open frontend:
   Open backend/frontend/recorder.html in a web browser

WORKFLOW:

1. Click "Get Prompt" to start a session
2. Read the prompt aloud (e.g., "What is this object?")
3. Click "Start Recording" and speak your answer
4. Recording stops automatically after silence or 8 seconds
5. View comprehensive analysis:
   - Transcript
   - Semantic evaluation (correct/partial/wrong)
   - Phoneme Error Rate (PER)
   - Problematic phonemes
   - Clinical insights
   - Detailed error table

6. Click "Get Prompt" again for another prompt with same object
7. Click "New Object" to switch to a different object


API ENDPOINTS
================================================================================

POST /session/start
  - Starts a new session
  - Returns: session_id, object, prompt_text, expected_answer

POST /session/next-prompt
  - Gets next prompt for current object
  - Params: session_id
  - Returns: prompt_text, expected_answer

POST /session/change-object
  - Changes to a different object
  - Params: session_id, new_object (optional)
  - Returns: new prompt for new object

POST /record
  - Uploads audio and performs analysis
  - Params: session_id, object_name, prompt_text, expected_answer, audio (file)
  - Returns: Complete analysis JSON:
      * transcript
      * wer, speech_rate, pause_ratio
      * semantic_evaluation
      * phoneme_analysis (NEW)
          - per
          - error_count
          - error_summary
          - problematic_phonemes
          - errors (detailed list)
          - clinical_notes


DATABASE SCHEMA
================================================================================

recordings table:
  - id (PRIMARY KEY)
  - session_id
  - object_name
  - prompt_text
  - expected_answer
  - audio_path
  - transcript
  - wer, speech_rate, pause_ratio
  - per, total_phonemes                    # ⭐ NEW
  - phoneme_errors_json                    # ⭐ NEW
  - problematic_phonemes_json              # ⭐ NEW
  - clinical_notes_json                    # ⭐ NEW
  - semantic_classification, semantic_score
  - created_at

patient_phoneme_history table:              # ⭐ NEW
  - id (PRIMARY KEY)
  - patient_id
  - phoneme
  - error_count
  - occurrence_count
  - last_error_date
  - created_at


KNOWN LIMITATIONS & FUTURE WORK
================================================================================

CURRENT LIMITATIONS:

1. No user authentication or patient management
2. Database saves are not yet implemented (TODO in code)
3. Gentle forced alignment is optional (works without it)
4. CMU Dict doesn't include all words (handles gracefully)
5. No GOP (Goodness of Pronunciation) scores yet
6. Frontend is single-user (no multi-patient interface)
7. No longitudinal progress visualization

FUTURE ENHANCEMENTS:

1. Implement database saving in routers/analysis.py
2. Add patient account management
3. Create therapist dashboard for progress tracking
4. Implement GOP scoring for pronunciation quality
5. Add more sophisticated VAD (Voice Activity Detection)
6. Support for additional languages
7. Real-time feedback during recording
8. Mobile app version
9. IRB compliance features (consent, anonymization)
10. Export to HIPAA-compliant formats


VALIDATION & TESTING
================================================================================

UNIT TESTS:
✓ Phoneme lookup (CMU Dict)
✓ PER calculation
✓ Phoneme alignment algorithm
✓ Error type classification
✓ Clinical note generation

INTEGRATION TESTING:
(To be implemented)
- End-to-end workflow testing
- API endpoint testing
- Database persistence testing

CLINICAL VALIDATION:
(Future work for publication)
- Inter-rater reliability with SLPs
- Comparison with gold-standard assessments
- Validation on aphasia patient data


ETHICAL CONSIDERATIONS
================================================================================

1. DATA PRIVACY:
   - Audio files stored locally
   - No cloud transmission by default
   - Database not encrypted (implement for production)

2. CLINICAL USE:
   - System is research-grade, not FDA-approved
   - Should supplement, not replace, SLP assessment
   - Requires informed consent for research use

3. IRB APPROVAL:
   - Required before patient data collection
   - Include data management plan
   - Obtain consent for audio recording

4. BIAS & FAIRNESS:
   - CMU Dict is primarily American English
   - May not accurately represent all dialects
   - Whisper model trained on diverse data


CITATION
================================================================================

If you use this system in your research, please cite:

[Your Name et al.] (2025). AI-Powered Speech Therapy Homework System with
Phoneme-Level Analysis for Aphasia Assessment. [Journal Name]. [DOI]


ACKNOWLEDGMENTS
================================================================================

This system uses the following open-source tools and datasets:

- CMU Pronouncing Dictionary (CMU Sphinx)
- Faster-Whisper (OpenAI Whisper optimized)
- Gentle Forced Aligner (lowerquality/gentle)
- Sentence Transformers (UKPLab)
- FastAPI framework


CONTACT & SUPPORT
================================================================================

For questions, bug reports, or collaboration inquiries:
- GitHub Issues: [repository URL]
- Email: [your email]

Last Updated: December 22, 2025
Version: 1.0.0
Status: Research MVP with Phoneme Analysis


================================================================================
                          END OF DOCUMENTATION
================================================================================
