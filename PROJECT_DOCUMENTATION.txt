================================================================================
           AI-POWERED SPEECH THERAPY HOMEWORK SYSTEM
            COMPREHENSIVE PROJECT DOCUMENTATION
================================================================================

PROJECT OVERVIEW
================================================================================
This is a research-grade speech therapy system designed for patients with
aphasia and speech disorders. The system provides automated speech analysis
at multiple levels: word-level, phoneme-level, semantic-level, and fluency.

Intended Use: Clinical research and speech therapy assessment
Target Population: Aphasia patients, dysarthria, and speech sound disorders
Research Status: Suitable for journal publication

KEY INNOVATION: Integration of pretrained ML models for acoustic-level
phoneme analysis, replacing basic text-based algorithms with state-of-the-art
deep learning models (Wav2Vec2, Silero VAD, Sentence Transformers).


SYSTEM ARCHITECTURE
================================================================================

+-------------------------------------------------------------------------+
|                           FRONTEND LAYER                                 |
|  +-------------------------------------------------------------------+  |
|  |  recorder.html - Browser-based Recording Interface                |  |
|  |  - MediaRecorder API for audio capture                            |  |
|  |  - Real-time silence detection (Web Audio API)                    |  |
|  |  - Result visualization with phoneme error highlighting           |  |
|  |                                                                    |  |
|  |  dashboard.html - Therapist Progress Dashboard                    |  |
|  |  - Patient progress visualization (Chart.js)                      |  |
|  |  - Longitudinal metrics tracking                                  |  |
|  +-------------------------------------------------------------------+  |
+---------------------------------------------------------------------------+
                                    | HTTP POST /record
                                    v
+-------------------------------------------------------------------------+
|                         BACKEND API LAYER                                |
|  +-------------------------------------------------------------------+  |
|  |  main.py - FastAPI Application                                    |  |
|  |  routers/analysis.py - API Endpoints                              |  |
|  |    - /session/start         - Initialize session                  |  |
|  |    - /session/next-prompt   - Get next prompt                     |  |
|  |    - /session/change-object - Switch object                       |  |
|  |    - /record                - Upload & analyze audio              |  |
|  |    - /generate-prompts      - LLM-powered prompt generation       |  |
|  +-------------------------------------------------------------------+  |
+---------------------------------------------------------------------------+
                                    |
                                    v
+-------------------------------------------------------------------------+
|                     ML-POWERED ANALYSIS PIPELINE                         |
|  +-------------------------------------------------------------------+  |
|  |  services/speech_processing.py - Main Pipeline Orchestrator       |  |
|  +-------------------------------------------------------------------+  |
|                                                                          |
|  +-------------+----------------+------------------+------------------+  |
|  |    ASR      |  ML Phoneme    |  ML Fluency     |  ML Semantic     |  |
|  |  (Whisper)  |  (Wav2Vec2)    |  (Silero VAD)   |  (Transformers)  |  |
|  +-------------+----------------+------------------+------------------+  |
|        |              |                 |                  |             |
|        v              v                 v                  v             |
|  Transcription   GOP Scores,       VAD-based         Similarity,        |
|  + Timestamps    IPA Phonemes,     Pause Detection   Paraphasia         |
|                  PER (ML)          Stutter Detection Classification     |
+---------------------------------------------------------------------------+
                                    |
                                    v
+-------------------------------------------------------------------------+
|                        DATA PERSISTENCE                                  |
|  +-------------------------------------------------------------------+  |
|  |  SQLite Database (speechtherapy.db)                               |  |
|  |  - recordings table: All analysis results + ML metrics            |  |
|  |  - patient_phoneme_history: Longitudinal phoneme tracking         |  |
|  |  - generated_prompts: LLM prompt cache                            |  |
|  |  - session_progress: Session-level aggregates                     |  |
|  |  - patient_baselines: Baseline metrics per patient                |  |
|  +-------------------------------------------------------------------+  |
+---------------------------------------------------------------------------+


SUCCESSFULLY IMPLEMENTED FEATURES
================================================================================

1. AUTOMATIC SPEECH RECOGNITION (ASR)
   [x] Faster-Whisper model integration (small/medium models)
   [x] CPU-optimized inference (int8 quantization)
   [x] Word-level timestamps with confidence scores
   [x] Multi-format audio support (webm, wav, mp3)
   [x] PyAV-based audio conversion (bundled FFmpeg)
   [x] English-only transcription to prevent language detection errors
   [x] SONIVA model support (fine-tuned for aphasia speech) - optional

2. WORD-LEVEL METRICS
   [x] Word Error Rate (WER) calculation using jiwer
   [x] Speech rate (words per minute)
   [x] Pause ratio (silence percentage)
   [x] Segment-level analysis

3. ML-BASED PHONEME ANALYSIS (NEW - Wav2Vec2)
   [x] Wav2Vec2 acoustic phoneme recognition (facebook/wav2vec2-lv-60-espeak-cv-ft)
   [x] Direct audio-to-IPA phoneme mapping
   [x] IPA to ARPAbet conversion for consistent comparison
   [x] Goodness of Pronunciation (GOP) scores per phoneme
   [x] ML-based Phoneme Error Rate (PER_ML)
   [x] Dynamic programming alignment (expected vs detected)
   [x] Model confidence scores
   [x] Frame-level probability analysis
   [x] Database persistence of all ML phoneme metrics

4. TEXT-BASED PHONEME ANALYSIS (Enhanced)
   [x] CMU Pronouncing Dictionary integration (134,000+ words)
   [x] Weighted Phoneme Error Rate (WPER) with phonetic distance
   [x] Phonetic feature-based distance calculation
   [x] Error type classification:
      - Substitutions (phonetically similar vs dissimilar)
      - Deletions (stressed vs unstressed)
      - Insertions
      - Metathesis (transposition)
   [x] Conduite d'approche detection (progressive improvement)
   [x] Multi-attempt segmentation and analysis
   [x] Phoneme class error tracking (fricatives, stops, vowels, etc.)
   [x] Clinical insights generation based on error patterns

5. ENHANCED SEMANTIC ANALYSIS
   [x] Sentence transformer-based similarity (all-MiniLM-L6-v2)
   [x] Multi-tier classification:
      - Correct (high similarity)
      - Partial (moderate similarity)
      - Wrong (low similarity)
      - Circumlocution (descriptive workaround)
      - Semantic paraphasia (related word substitution)
   [x] WordNet-based category similarity
   [x] Object feature matching for circumlocution detection
   [x] Paraphasia type identification (semantic, phonemic)
   [x] Clinical significance notes

6. ML-BASED FLUENCY & STUTTERING ANALYSIS (Silero VAD)
   [x] Silero VAD for voice activity detection
   [x] ML-based pause detection and classification:
      - Hesitations (300ms - 1s)
      - Blocks (>1s)
      - Anomic pauses (word-finding difficulty)
      - Apraxic pauses (motor planning)
   [x] ML-based stuttering detection:
      - Repetitions (word/sound level)
      - Prolongations
      - Interjections
      - Blocks
   [x] Longest Fluent Run (LFR) with tolerance
   [x] SSI-4 approximation (Stuttering Severity Instrument)
   [x] Speech rate metrics:
      - Articulation rate (excluding pauses)
      - Speaking rate (including pauses)
      - Phonation time and ratio
   [x] Rate variability analysis (CV, PVI, trend)
   [x] Weighted fluency scoring

7. DATA PERSISTENCE & SESSION MANAGEMENT
   [x] Automatic database saving of all analysis results
   [x] ML phoneme analysis persistence (GOP, PER_ML, alignment)
   [x] Session tracking (grouping multiple prompts)
   [x] Patient identification and longitudinal tracking
   [x] Comprehensive query functions:
      - Patient recording history with filtering
      - Progress tracking over time (trends)
      - Phoneme trend analysis
      - Session summaries with aggregate metrics
   [x] Patient phoneme history tracking
   [x] API endpoints for therapist insights

8. DYNAMIC PROMPT GENERATION (LLM-Powered)
   [x] Ollama integration (zero-cost, local, privacy-preserving)
   [x] Default model: gemma2:2b (lightweight, fast)
   [x] Support for ANY object (unlimited vocabulary)
   [x] Contextually appropriate question generation
   [x] Multiple expected answers per question
   [x] Database caching for instant reuse
   [x] Graceful fallback to template-based prompts
   [x] Real-time LLM status indicators in UI

9. THERAPIST DASHBOARD
   [x] Patient progress visualization
   [x] Interactive Chart.js visualizations
   [x] Clinical insights panel
   [x] Summary metric cards
   [x] Recent sessions table with expandable details
   [x] Patient selector with real-time data loading

10. USER INTERFACE (Patient/Recorder)
    [x] Browser-based audio recording
    [x] Automatic silence detection (VAD)
    [x] Comprehensive result display
    [x] Collapsible detailed views
    [x] Session management


TECHNICAL SPECIFICATIONS
================================================================================

BACKEND STACK:
  - Framework: FastAPI
  - ASGI Server: Uvicorn
  - Database: SQLite with SQLAlchemy ORM
  - ML Models:
      * Faster-Whisper (small/medium) for ASR
      * Wav2Vec2 (facebook/wav2vec2-lv-60-espeak-cv-ft) for phoneme recognition
      * Silero VAD for voice activity detection
      * Sentence-Transformers (all-MiniLM-L6-v2) for semantics
  - Phoneme Analysis:
      * CMU Pronouncing Dictionary (134,000+ words)
      * phonemizer + espeak-ng for G2P
      * Edit distance algorithms with phonetic weighting

FRONTEND STACK:
  - Pure HTML5 + Vanilla JavaScript (no frameworks)
  - Web APIs:
      * MediaRecorder API (audio capture)
      * Web Audio API (silence detection)
      * Fetch API (HTTP communication)
  - Chart.js for data visualization

AUDIO PROCESSING:
  - Supported formats: webm, wav, mp3, ogg, flac
  - PyAV for audio conversion (bundled FFmpeg)
  - Silero VAD for speech segment detection
  - Auto-stop: 900ms silence or 8s timeout

KEY DEPENDENCIES (requirements.txt):
  - fastapi, uvicorn
  - sqlalchemy, python-multipart
  - faster-whisper
  - torch, torchaudio
  - transformers (Wav2Vec2)
  - sentence-transformers
  - silero-vad
  - phonemizer
  - numpy, scipy
  - noisereduce
  - jiwer, editdistance
  - av (PyAV for audio conversion)
  - nltk (WordNet, CMUdict)


FILE STRUCTURE
================================================================================

FINAL PROJECT/
+-- backend/
|   +-- main.py                      # FastAPI application entry point
|   +-- config.py                    # Configuration (DB_URL, model paths)
|   +-- requirements.txt             # Python dependencies
|   |
|   +-- routers/
|   |   +-- analysis.py              # API endpoints
|   |
|   +-- services/
|   |   +-- speech_processing.py     # Main analysis pipeline orchestrator
|   |   +-- semantic_analysis.py     # Basic semantic evaluation
|   |   +-- enhanced_semantic_analysis.py  # Advanced semantic with paraphasia
|   |   +-- phoneme_analysis.py      # Basic phoneme analysis
|   |   +-- enhanced_phoneme_analysis.py   # WPER, conduite d'approche
|   |   +-- phoneme_features.py      # Phonetic feature distances
|   |   +-- phoneme_mapping.py       # IPA <-> ARPAbet conversion
|   |   +-- phoneme_lookup.py        # CMU Dict wrapper
|   |   +-- ml_phoneme.py            # Wav2Vec2 phoneme recognition
|   |   +-- ml_models.py             # ML model registry/loader
|   |   +-- ml_vad.py                # Silero VAD wrapper
|   |   +-- ml_stutter.py            # ML-based stuttering detection
|   |   +-- fluency_analysis.py      # Basic fluency analysis
|   |   +-- enhanced_fluency_analysis.py   # SSI-4, rate variability
|   |   +-- llm_prompt_generator.py  # Ollama LLM integration
|   |   +-- prompts.py               # Legacy hardcoded prompts
|   |   +-- metrics.py               # WER, rate, pause calculations
|   |   +-- audio.py                 # Audio preprocessing
|   |   +-- forced_alignment.py      # Gentle integration (optional)
|   |
|   +-- models/
|   |   +-- whisper_model.py         # Whisper ASR singleton
|   |   +-- cache/                   # Hugging Face model cache
|   |
|   +-- database/
|   |   +-- db.py                    # SQLAlchemy engine
|   |   +-- schema.py                # Table initialization
|   |   +-- schema.sql               # Full SQL schema
|   |   +-- persistence.py           # Save/query functions
|   |   +-- migrate_ml_phoneme.py    # Migration for ML columns
|   |   +-- migrate_enhanced_schema.py  # Migration for enhanced analysis
|   |
|   +-- data/
|   |   +-- objects.py               # Object definitions
|   |   +-- cmudict-0.7b.txt         # CMU Pronouncing Dictionary
|   |
|   +-- utils/
|   |   +-- file_utils.py            # File management
|   |   +-- audio_converter.py       # PyAV audio conversion
|   |
|   +-- frontend/
|   |   +-- recorder.html            # Patient recording interface
|   |   +-- dashboard.html           # Therapist progress dashboard
|   |
|   +-- storage/
|   |   +-- audio/                   # Audio recordings (gitignored)
|   |
|   +-- tests/
|       +-- test_phoneme_analysis.py
|       +-- test_fluency_analysis.py
|       +-- test_persistence.py
|
+-- docker-compose.yml               # Gentle service configuration
+-- OLLAMA_SETUP.md                  # LLM installation guide
+-- TESTING_GUIDE.md                 # Dashboard testing guide
+-- project_documentation.txt        # This file


KEY ALGORITHMS
================================================================================

1. ML-BASED PHONEME RECOGNITION (Wav2Vec2):

   Model: facebook/wav2vec2-lv-60-espeak-cv-ft
   Output: IPA phoneme sequence from raw audio

   Process:
   1. Load audio, resample to 16kHz
   2. Extract features using Wav2Vec2FeatureExtractor
   3. Run through Wav2Vec2ForCTC model
   4. Decode logits to IPA phonemes via CTC
   5. Convert IPA to ARPAbet for comparison with CMUdict
   6. Calculate GOP from frame probabilities

2. GOODNESS OF PRONUNCIATION (GOP):

   GOP = log P(phoneme | acoustic features)

   Calculation:
   1. Get frame-level softmax probabilities from model
   2. For each phoneme segment, average max probabilities
   3. Take log probability as GOP score
   4. Score near 0 = good pronunciation
   5. Score << 0 = poor pronunciation

3. IPA TO ARPABET CONVERSION:

   Wav2Vec2 outputs IPA symbols, CMUdict uses ARPAbet.
   phoneme_mapping.py provides bidirectional conversion.

   Example:
   IPA:     [a, r, a, h, i, u:, s, t, o, b, a, k]
   ARPAbet: [AA, R, AA, HH, IY, UW, S, T, OW, B, AA, K]

4. WEIGHTED PHONEME ERROR RATE (WPER):

   WPER = Sum(error_weights) / N

   Weights based on phonetic distance:
   - Phonetically similar substitution: 0.3
   - Phonetically dissimilar substitution: 0.7
   - Deletion (unstressed): 0.4
   - Deletion (stressed): 0.8
   - Insertion: 0.6
   - Metathesis: 0.5

5. PHONETIC DISTANCE:

   Based on articulatory features:
   - Voicing (voiced/voiceless): 15%
   - Place of articulation: 25%
   - Manner of articulation: 30%
   - Vowel height: 15%
   - Vowel backness: 15%

6. SILERO VAD FOR PAUSE DETECTION:

   Model: silero-vad (snakers4/silero-vad)

   Process:
   1. Run VAD on audio to get speech probabilities
   2. Identify speech vs non-speech segments
   3. Calculate pause durations between speech segments
   4. Classify pauses by duration and context:
      - Hesitation: 300ms - 1000ms
      - Block: >1000ms
      - Anomic: Before content word
      - Apraxic: Multiple attempts pattern

7. SSI-4 APPROXIMATION:

   Stuttering Severity Instrument scoring:
   - Frequency: % of syllables stuttered
   - Duration: Average stutter duration
   - Total score maps to severity level

8. SEMANTIC PARAPHASIA DETECTION:

   Uses WordNet for category analysis:
   1. Get hypernym chains for expected and produced words
   2. Find lowest common ancestor
   3. Calculate semantic distance
   4. Classify: semantic (related category) vs phonemic (sound-alike)


DATABASE SCHEMA
================================================================================

recordings table:
  - id (PRIMARY KEY)
  - session_id, patient_id
  - object_name, prompt_text, expected_answer
  - audio_path, transcript

  # Word-level metrics
  - wer, speech_rate, pause_ratio

  # Text-based phoneme metrics
  - per, wper, total_phonemes
  - phoneme_errors_json, problematic_phonemes_json
  - phoneme_class_errors_json, error_pattern_analysis_json
  - conduite_d_approche_detected, multi_attempt_analysis_json
  - clinical_notes_json

  # ML Phoneme metrics (NEW)
  - ml_per                       # ML-based PER
  - ml_gop                       # Overall GOP score
  - ml_confidence                # Model confidence
  - ml_detected_phonemes_json    # Detected ARPAbet sequence
  - ml_detected_ipa_json         # Detected IPA sequence
  - ml_alignment_json            # (expected, detected) pairs
  - ml_phoneme_scores_json       # Per-phoneme GOP scores

  # Semantic metrics
  - semantic_classification, semantic_score
  - direct_similarity, category_similarity
  - circumlocution_detected, circumlocution_features_json
  - semantic_paraphasia_json

  # Enhanced fluency metrics
  - longest_fluent_run, lfr_with_tolerance, lfr_ratio
  - total_pauses, hesitation_count, block_count
  - anomic_pause_count, apraxic_pause_count
  - mean_pause_duration
  - articulation_rate, speaking_rate, pause_adjusted_rate
  - phonation_time, phonation_ratio, pathological_pause_ratio
  - local_cv, global_cv, normalized_pvi, rate_trend
  - ssi_frequency_pct, ssi_frequency_score
  - ssi_avg_duration, ssi_duration_score
  - ssi_total_score, ssi_severity
  - standard_fluency_pct, weighted_fluency_pct, clinical_fluency_pct
  - stuttering_events_json, pauses_json, fluency_notes_json
  - dysfluency_profile_json

  # Metadata
  - analysis_mode, disorder_type, created_at

patient_phoneme_history table:
  - id, patient_id, phoneme
  - error_count, occurrence_count
  - last_error_date, created_at

generated_prompts table:
  - id, object_name (UNIQUE)
  - prompts_json, model_name, generation_method
  - created_at, last_used_at

session_progress table:
  - id, session_id (UNIQUE), patient_id
  - start_time, end_time
  - total_recordings
  - avg_wper, avg_per, avg_weighted_fluency, avg_lfr, avg_ssi_score
  - objects_practiced, clinical_summary_json

patient_baselines table:
  - id, patient_id (UNIQUE), disorder_type
  - baseline_per, baseline_wper, baseline_lfr, baseline_fluency
  - baseline_articulation_rate
  - mean_pause_duration, std_pause_duration, cv_rate
  - baseline_date, notes


API ENDPOINTS
================================================================================

POST /record
  - Main analysis endpoint
  - Returns comprehensive analysis including ML phoneme metrics:
      * ml_analysis: {
          detected_phonemes: [],      # ARPAbet
          detected_phonemes_ipa: [],  # IPA
          expected_phonemes: [],      # ARPAbet from CMUdict
          per_ml: float,              # ML-based PER
          overall_gop: float,         # GOP score
          model_confidence: float,
          alignment: [(exp, det), ...],
          phoneme_scores: [{phoneme, gop_score, ...}, ...]
        }

GET /patient/{patient_id}/history
  - Returns recordings with ML metrics included

GET /patient/{patient_id}/progress
  - Aggregated progress including ML metrics

POST /generate-prompts
  - LLM-powered prompt generation

GET /ollama/status
  - Check LLM availability


RESEARCH APPLICATIONS
================================================================================

This system enables research on:

1. Acoustic phoneme analysis in disordered speech
2. GOP-based pronunciation assessment in aphasia
3. Longitudinal tracking with objective ML metrics
4. Comparison of text-based vs acoustic phoneme analysis
5. Semantic paraphasia patterns and recovery
6. Fluency disorder progression with VAD-based metrics
7. SSI-4 validation with automated scoring
8. Multi-modal speech assessment (acoustic + linguistic)


USAGE INSTRUCTIONS
================================================================================

SETUP:

1. Install Python dependencies:
   cd backend
   pip install -r requirements.txt

2. Install espeak-ng (required for phonemizer):
   Windows: winget install eSpeak-NG.eSpeak-NG
   Linux: apt-get install espeak-ng
   Mac: brew install espeak-ng

3. (Optional) Install Ollama for LLM prompts:
   See OLLAMA_SETUP.md

4. Start the backend server:
   cd backend
   uvicorn main:app --reload --port 8000

5. Open frontend:
   http://localhost:8000 (Patient Interface)
   http://localhost:8000/dashboard (Therapist Dashboard)


KNOWN LIMITATIONS & FUTURE WORK
================================================================================

CURRENT LIMITATIONS:

1. No user authentication
2. Session storage is in-memory
3. SONIVA Whisper model not bundled (optional download)
4. espeak-ng required for ML phoneme analysis

FUTURE ENHANCEMENTS:

1. Real-time pronunciation feedback
2. Mobile app version
3. Multi-language support
4. HIPAA-compliant deployment
5. Integration with EHR systems


CITATION
================================================================================

If you use this system in your research, please cite:

[Your Name et al.] (2026). AI-Powered Speech Therapy System with
ML-Based Phoneme Analysis for Aphasia Assessment. [Journal Name].


ACKNOWLEDGMENTS
================================================================================

This system uses the following open-source tools:

- Wav2Vec2 (Facebook AI Research)
- Silero VAD (silero-team)
- Faster-Whisper (SYSTRAN)
- Sentence Transformers (UKPLab)
- CMU Pronouncing Dictionary
- phonemizer + espeak-ng
- Ollama (local LLM runtime)
- FastAPI, Chart.js


Last Updated: January 13, 2026
Version: 4.0.0
Status: Research MVP with ML-Powered Analysis
Features: Wav2Vec2 Phoneme | Silero VAD | Enhanced Semantics | Database Persistence

================================================================================
                          END OF DOCUMENTATION
================================================================================
