================================================================================
           AI-POWERED SPEECH THERAPY HOMEWORK SYSTEM
            COMPREHENSIVE PROJECT DOCUMENTATION
================================================================================

PROJECT OVERVIEW
================================================================================
This is a research-grade speech therapy system designed for patients with
aphasia and speech disorders. The system provides automated speech analysis
at multiple levels: word-level, phoneme-level, semantic-level, and fluency.

Intended Use: Clinical research and speech therapy assessment
Target Population: Aphasia patients, dysarthria, and speech sound disorders
Research Status: Suitable for journal publication

KEY INNOVATION: Integration of pretrained ML models for acoustic-level
phoneme analysis, replacing basic text-based algorithms with state-of-the-art
deep learning models (Wav2Vec2, Silero VAD, Sentence Transformers).


================================================================================
                    HOW THE SYSTEM WORKS - DETAILED FLOW
================================================================================

STEP 1: PATIENT RECORDING SESSION
--------------------------------------------------------------------------------
When a patient starts a session:

OPTION A: Manual Recording Interface (recorder.html)
1. Patient opens the web interface (http://localhost:8001)
2. Optionally enters their Patient ID for longitudinal tracking
3. Selects an object to practice (e.g., "bag", "cup", "phone")
   - Objects can be predefined or custom (LLM generates prompts for any object)
4. System displays a prompt question (e.g., "What do you carry things in?")
5. Patient clicks "Record" and speaks their answer
6. Browser captures audio using MediaRecorder API (WebM format)
7. Client-side VAD detects 900ms of silence → auto-stops recording
8. Audio blob is sent to backend via POST /record

OPTION B: AR Camera Interface (ar.html) - NEW!
1. Patient opens the AR interface (http://localhost:8001/ar)
2. Enters their Patient ID for session tracking
3. Allows camera access (uses rear camera on mobile)
4. Points camera at a real-world object
5. Clicks "Detect Object" - YOLOv8 ML model identifies the object
   - Uses YOLOv8n ONNX model running in-browser via ONNX Runtime Web
   - Supports 80 COCO classes (bottle, cup, phone, book, etc.)
   - Draws bounding box with confidence score on detected object
6. Clicks "Generate Prompts" - System creates prompts for detected object
7. Prompt overlay displays on camera feed
8. Patient speaks their answer while recording (auto-starts)
9. Audio auto-stops after 900ms silence or 8s timeout
10. Clicks "Next Prompt" to cycle through prompts
11. Clicks "New Object" to detect a different object


STEP 2: AUDIO PREPROCESSING
--------------------------------------------------------------------------------
Backend receives audio and prepares it for analysis:

1. AUDIO CONVERSION (utils/audio_converter.py)
   - Input: WebM blob from browser
   - PyAV (bundled FFmpeg) converts to WAV format
   - Resamples to 16kHz mono (required by ML models)
   - Saves to storage/audio/{session_id}_{timestamp}.wav

2. NOISE REDUCTION (optional)
   - noisereduce library for spectral gating
   - Reduces background noise while preserving speech


STEP 3: AUTOMATIC SPEECH RECOGNITION (ASR)
--------------------------------------------------------------------------------
Whisper model transcribes speech to text:

1. MODEL LOADING (models/whisper_model.py)
   - Uses faster-whisper for CPU-optimized inference
   - Model: whisper-small (or medium for better accuracy)
   - INT8 quantization for faster CPU inference

2. TRANSCRIPTION PROCESS
   - Audio → Whisper → Transcript + Word-level timestamps
   - Forces English language (prevents false language detection)
   - Returns: {
       "text": "I use the back",
       "segments": [...],
       "word_timings": [
         {"word": "I", "start": 0.0, "end": 0.2},
         {"word": "use", "start": 0.3, "end": 0.5},
         ...
       ]
     }

3. WORD ERROR RATE (WER) CALCULATION
   - Compares transcript to expected answer using jiwer
   - WER = (Substitutions + Deletions + Insertions) / Total Words
   - Example: Expected "I use the bag", Got "I use the back"
     WER = 1/4 = 0.25 (25% error rate)


STEP 4: ML-BASED PHONEME ANALYSIS (Wav2Vec2)
--------------------------------------------------------------------------------
Analyzes pronunciation at the acoustic level:

1. MODEL: facebook/wav2vec2-lv-60-espeak-cv-ft
   - Trained on 60,000 hours of speech
   - Fine-tuned for phoneme recognition
   - Outputs IPA (International Phonetic Alphabet) symbols

2. ACOUSTIC PHONEME DETECTION PROCESS
   a) Load audio waveform (16kHz)
   b) Extract features using Wav2Vec2FeatureExtractor
   c) Pass through Wav2Vec2ForCTC model
   d) Get frame-level logits (probabilities for each phoneme)
   e) CTC decode to get phoneme sequence

   Example:
   Audio of "bag" → Model → [b, æ, ɡ] (IPA)

3. IPA TO ARPABET CONVERSION (phoneme_mapping.py)
   - Wav2Vec2 outputs IPA, CMUdict uses ARPAbet
   - Converts for consistent comparison

   IPA:     [b, æ, ɡ]
   ARPAbet: [B, AE, G]

4. GOODNESS OF PRONUNCIATION (GOP) SCORING
   - Measures how well each phoneme was pronounced
   - GOP = log P(phoneme | audio features)

   Calculation:
   a) Get softmax probabilities from model output
   b) For each phoneme, find frames where it was detected
   c) Average the max probabilities for those frames
   d) Take log to get GOP score

   Interpretation:
   - GOP ≈ 0: Good pronunciation (high confidence)
   - GOP << -2: Poor pronunciation (low confidence)

5. ML-BASED PHONEME ERROR RATE (PER_ML)
   - Aligns expected phonemes (from CMUdict) with detected phonemes
   - Uses dynamic programming (Needleman-Wunsch algorithm)
   - Counts substitutions, insertions, deletions

   Example:
   Expected: [DH, AH, B, AE, G]  ("the bag")
   Detected: [DH, AH, B, AE, K]  (detected from audio)
   Alignment: [(DH,DH), (AH,AH), (B,B), (AE,AE), (G,K)]
   PER_ML = 1/5 = 0.20 (one substitution: G→K)


STEP 5: RULE-BASED PHONEME ANALYSIS (Enhanced)
--------------------------------------------------------------------------------
Text-based analysis using linguistic rules:

1. CMU PRONOUNCING DICTIONARY LOOKUP
   - 134,000+ English words with phoneme transcriptions
   - Looks up expected and actual words

   "bag" → [B, AE1, G]
   "back" → [B, AE1, K]

2. PHONEME ALIGNMENT & ERROR DETECTION
   - Aligns expected vs actual phoneme sequences
   - Classifies each error by type:

   ERROR TYPES:
   - Substitution (phonetically similar): d→0.3 weight
     Example: [G] → [K] (both are velar stops)
   - Substitution (phonetically dissimilar): d→0.7 weight
     Example: [S] → [M] (fricative vs nasal)
   - Deletion (unstressed): 0.4 weight
   - Deletion (stressed): 0.8 weight
   - Insertion: 0.6 weight
   - Metathesis (transposition): 0.5 weight

3. WEIGHTED PHONEME ERROR RATE (WPER)
   - Unlike simple PER, weights errors by clinical significance
   - WPER = Sum(error_weights) / Total_phonemes

   Example: "bag" → "back"
   - G→K substitution, phonetically similar (both velars)
   - Weight = 0.3
   - WPER = 0.3/3 = 0.10 (vs PER = 0.33)

4. PHONETIC DISTANCE CALCULATION
   Based on articulatory features:

   CONSONANT FEATURES:
   - Voicing: voiced vs voiceless (15% weight)
   - Place: bilabial, alveolar, velar, etc. (25% weight)
   - Manner: stop, fricative, nasal, etc. (30% weight)

   VOWEL FEATURES:
   - Height: high, mid, low (15% weight)
   - Backness: front, central, back (15% weight)

   Example: [G] vs [K]
   - Same place (velar): 0
   - Same manner (stop): 0
   - Different voicing: 0.15
   - Distance = 0.15 (very similar)

5. CONDUITE D'APPROCHE DETECTION
   - Pattern where patient makes multiple attempts, getting closer
   - Example: "b...buh...ba...bag"
   - Detected by analyzing word timing segments
   - Indicates preserved phonological processing


STEP 6: SEMANTIC ANALYSIS
--------------------------------------------------------------------------------
Evaluates meaning and word-finding:

1. SENTENCE SIMILARITY (Sentence Transformers)
   - Model: all-MiniLM-L6-v2
   - Encodes expected and actual sentences to vectors
   - Calculates cosine similarity

   Example:
   Expected: "I use the bag"
   Actual: "I use the back"
   Similarity: 0.85 (high, mostly correct)

2. CLASSIFICATION TIERS
   Based on similarity score:
   - CORRECT: similarity > 0.85
   - PARTIAL: 0.5 < similarity < 0.85
   - WRONG: similarity < 0.5
   - CIRCUMLOCUTION: Low similarity but describes object
   - SEMANTIC_PARAPHASIA: Related word substitution

3. SEMANTIC PARAPHASIA DETECTION
   - Identifies when patient substitutes a related word
   - Uses position-based matching:

   Expected: "I use the bag"
   Actual:   "I use the back"

   Process:
   a) Find target word position in expected ("bag" at position 3)
   b) Get word at same position in actual ("back" at position 3)
   c) Compare "bag" vs "back"
   d) Check if phonetically similar → Phonological paraphasia
   e) Check if semantically related → Semantic paraphasia

   Types:
   - PHONOLOGICAL: Sound-alike substitution (bag→back)
   - SEMANTIC: Category-related (bag→purse, dog→cat)
   - UNRELATED: No clear relationship

4. CIRCUMLOCUTION DETECTION
   - Identifies descriptive workarounds
   - Example: "the thing you carry stuff in" for "bag"
   - Uses feature matching and object descriptions
   - Common in anomic aphasia


STEP 7: FLUENCY ANALYSIS (ML-Based)
--------------------------------------------------------------------------------
Analyzes speech flow and dysfluencies:

1. SILERO VAD (Voice Activity Detection)
   - Model: snakers4/silero-vad
   - Detects speech vs silence with high precision
   - Returns speech probability per frame

   Process:
   a) Run VAD on entire audio
   b) Get speech segments: [(0.0, 0.5), (0.8, 1.2), ...]
   c) Calculate gaps between segments (pauses)

2. PAUSE CLASSIFICATION
   By duration:
   - HESITATION: 300ms - 1000ms (word-finding delay)
   - BLOCK: > 1000ms (motor planning difficulty)

   By context:
   - ANOMIC PAUSE: Before content word (noun, verb)
   - APRAXIC PAUSE: Multiple short pauses (motor planning)
   - FORMULATION PAUSE: Before complex phrase

   Example Analysis:
   "I want... [800ms pause] ...the bag"
   → Hesitation pause, likely anomic (before noun "bag")

3. STUTTERING DETECTION
   Identifies dysfluency patterns:

   - REPETITION: "b-b-b-bag" (sound/syllable repeat)
   - PROLONGATION: "sssssnake" (sound stretching)
   - BLOCK: "[silence]...bag" (inability to initiate)
   - INTERJECTION: "um", "uh", "like"

4. SPEECH RATE METRICS
   - Articulation Rate: syllables/second (excluding pauses)
   - Speaking Rate: syllables/second (including pauses)
   - Pause Ratio: pause_time / total_time

   Normal ranges:
   - Articulation rate: 4-6 syllables/second
   - Speaking rate: 2-4 syllables/second

5. LONGEST FLUENT RUN (LFR)
   - Maximum words spoken without dysfluency
   - Key metric for fluency disorders
   - LFR with tolerance allows minor hesitations

6. SSI-4 APPROXIMATION (Stuttering Severity Instrument)
   Calculates severity score:

   Components:
   - Frequency: % of syllables with stuttering
   - Duration: Average stutter duration in seconds

   Scoring:
   - Frequency < 1%: 2 points
   - Frequency 1-3%: 4 points
   - Frequency 4-6%: 6 points
   - ... (scales up)

   Severity levels:
   - 0-10: Normal
   - 11-20: Mild
   - 21-30: Moderate
   - 31-40: Severe
   - 41+: Very Severe


STEP 8: RESULT AGGREGATION
--------------------------------------------------------------------------------
Combines all analyses into unified response:

{
  "transcript": "I use the back",
  "analysis_mode": "enhanced",

  "phoneme_analysis": {
    "per_rule": 0.333,        // Rule-based PER
    "wper": 0.100,            // Weighted PER
    "per_ml": 0.200,          // ML-based PER
    "overall_gop": -0.45,     // Pronunciation quality
    "errors": [
      {
        "type": "substitution",
        "expected": "G",
        "actual": "K",
        "phonetic_distance": 0.15,
        "clinical_significance": "Minor - phonetically similar"
      }
    ],
    "clinical_notes": [
      "Substitution pattern detected",
      "Errors are phonetically similar - phonological system preserved"
    ]
  },

  "semantic_evaluation": {
    "classification": "partial",
    "similarity_score": 0.85,
    "semantic_paraphasia": {
      "detected": true,
      "type": "phonological",
      "target_word": "bag",
      "produced_word": "back"
    }
  },

  "fluency_analysis": {
    "longest_fluent_run": 4,
    "total_pauses": 1,
    "hesitation_count": 1,
    "speech_rate": 2.5,
    "ssi_severity": "normal"
  },

  "summary": {
    "per_rule": 0.333,
    "per_ml": 0.200,
    "wper": 0.100,
    "semantic_classification": "partial",
    "semantic_paraphasia_detected": true
  }
}


STEP 9: DATABASE PERSISTENCE
--------------------------------------------------------------------------------
All results are saved for longitudinal tracking:

1. RECORDING STORAGE
   - Full analysis JSON saved to recordings table
   - Linked to session_id and patient_id
   - Timestamps for temporal analysis

2. PHONEME HISTORY TRACKING
   - Tracks problematic phonemes per patient
   - Enables identification of persistent errors
   - Supports targeted therapy planning

3. SESSION AGGREGATION
   - Calculates session-level averages
   - Tracks progress within sessions
   - Identifies improvement patterns


STEP 10: THERAPIST DASHBOARD
--------------------------------------------------------------------------------
Visualizes patient progress:

1. PATIENT SELECTION
   - Input patient ID to load data
   - Loads all historical recordings and metrics

2. METRIC CARDS
   - Total Recordings count
   - Average PER (Phoneme Error Rate)
   - Average LFR (Longest Fluent Run)
   - Average Fluency Percentage

3. PROGRESS CHARTS (Chart.js)
   - LFR Trend over time (line chart)
   - PER Trend over time (line chart)
   - Most Problematic Phonemes (bar chart)
   - Fluency Percentage Trend (line chart)

4. CLINICAL INSIGHTS PANEL
   - Auto-generated observations based on data trends
   - LFR improvement/decline detection
   - Focus areas (high-error phonemes)
   - Fluency level assessment
   - Object-specific performance comparison

5. SESSIONS TABLE
   - Expandable rows for each recording
   - Shows: date, object, transcript, metrics
   - "View Full Analysis" button → links to analysis.html


STEP 11: DETAILED ANALYSIS PAGE (NEW)
--------------------------------------------------------------------------------
Standalone page for deep-dive analysis of individual recordings:

1. ACCESS
   - URL: /analysis?recording_id={id}
   - Linked from dashboard "View Full Analysis" button

2. SECTIONS DISPLAYED
   - Transcript with prompt and expected answer
   - Semantic Evaluation (classification + similarity score)
   - Rule-Based Phoneme Analysis (PER, error table, problematic phonemes)
   - ML Phoneme Analysis (PER_ML, GOP scores, detected phonemes)
   - Fluency Analysis (LFR, pauses, SSI-4, stuttering events)
   - Clinical Notes aggregation
   - Session metadata


================================================================================
                         SYSTEM ARCHITECTURE
================================================================================

+-------------------------------------------------------------------------+
|                           FRONTEND LAYER                                 |
|  +-------------------------------------------------------------------+  |
|  |  recorder.html - Browser-based Recording Interface                |  |
|  |  - MediaRecorder API for audio capture                            |  |
|  |  - Real-time silence detection (Web Audio API)                    |  |
|  |  - Result visualization with phoneme error highlighting           |  |
|  |                                                                    |  |
|  |  ar.html - AR Object Detection + Speech Practice (NEW!)           |  |
|  |  - YOLOv8 ONNX model for real-time object detection               |  |
|  |  - Camera feed with bounding box overlay                          |  |
|  |  - Integrated prompt generation and speech recording              |  |
|  |  - Supports 80 COCO object classes                                |  |
|  |                                                                    |  |
|  |  dashboard.html - Therapist Progress Dashboard                    |  |
|  |  - Patient progress visualization (Chart.js)                      |  |
|  |  - Longitudinal metrics tracking                                  |  |
|  |  - Clinical insights generation                                   |  |
|  |                                                                    |  |
|  |  analysis.html - Detailed Recording Analysis (NEW!)               |  |
|  |  - Full analysis breakdown for individual recordings              |  |
|  |  - Phoneme, semantic, fluency sections                            |  |
|  +-------------------------------------------------------------------+  |
+---------------------------------------------------------------------------+
                                    | HTTP POST /record
                                    v
+-------------------------------------------------------------------------+
|                         BACKEND API LAYER                                |
|  +-------------------------------------------------------------------+  |
|  |  main.py - FastAPI Application                                    |  |
|  |  routers/analysis.py - API Endpoints                              |  |
|  |    - /session/start         - Initialize session                  |  |
|  |    - /session/next-prompt   - Get next prompt                     |  |
|  |    - /session/change-object - Switch object                       |  |
|  |    - /record                - Upload & analyze audio              |  |
|  |    - /generate-prompts      - LLM-powered prompt generation       |  |
|  |    - /recording/{id}        - Get single recording details (NEW)  |  |
|  |    - /ar                    - AR interface page (NEW)             |  |
|  |    - /analysis              - Analysis page (NEW)                 |  |
|  +-------------------------------------------------------------------+  |
+---------------------------------------------------------------------------+
                                    |
                                    v
+-------------------------------------------------------------------------+
|                     ML-POWERED ANALYSIS PIPELINE                         |
|  +-------------------------------------------------------------------+  |
|  |  services/speech_processing.py - Main Pipeline Orchestrator       |  |
|  +-------------------------------------------------------------------+  |
|                                                                          |
|  +-------------+----------------+------------------+------------------+  |
|  |    ASR      |  ML Phoneme    |  ML Fluency     |  ML Semantic     |  |
|  |  (Whisper)  |  (Wav2Vec2)    |  (Silero VAD)   |  (Transformers)  |  |
|  +-------------+----------------+------------------+------------------+  |
|        |              |                 |                  |             |
|        v              v                 v                  v             |
|  Transcription   GOP Scores,       VAD-based         Similarity,        |
|  + Timestamps    IPA Phonemes,     Pause Detection   Paraphasia         |
|                  PER (ML)          Stutter Detection Classification     |
+---------------------------------------------------------------------------+
                                    |
                                    v
+-------------------------------------------------------------------------+
|                        DATA PERSISTENCE                                  |
|  +-------------------------------------------------------------------+  |
|  |  SQLite Database (speechtherapy.db)                               |  |
|  |  - recordings table: All analysis results + ML metrics            |  |
|  |  - patient_phoneme_history: Longitudinal phoneme tracking         |  |
|  |  - generated_prompts: LLM prompt cache                            |  |
|  |  - session_progress: Session-level aggregates                     |  |
|  |  - patient_baselines: Baseline metrics per patient                |  |
|  +-------------------------------------------------------------------+  |
+---------------------------------------------------------------------------+


================================================================================
                    KEY METRICS EXPLAINED
================================================================================

PHONEME ERROR RATE (PER)
--------------------------------------------------------------------------------
Measures pronunciation accuracy at the phoneme level.

Formula: PER = (S + D + I) / N

Where:
- S = Substitutions (wrong phoneme)
- D = Deletions (missing phoneme)
- I = Insertions (extra phoneme)
- N = Total expected phonemes

Example:
Expected: /B AE G/ (bag)
Actual:   /B AE K/ (back)
PER = 1/3 = 0.333 (33% error)

Two versions in this system:
- per_rule: Calculated from text (CMUdict lookup)
- per_ml: Calculated from audio (Wav2Vec2 detection)


WEIGHTED PHONEME ERROR RATE (WPER)
--------------------------------------------------------------------------------
Like PER but weights errors by clinical significance.

Formula: WPER = Sum(w_i) / N

Where w_i is the weight for each error:
- Phonetically similar substitution: 0.3
- Phonetically dissimilar substitution: 0.7
- Unstressed deletion: 0.4
- Stressed deletion: 0.8
- Insertion: 0.6
- Metathesis: 0.5

Clinical rationale:
- Similar sound errors indicate intact phonological system
- Dissimilar errors suggest more severe impairment
- Stressed syllable errors are more noticeable


GOODNESS OF PRONUNCIATION (GOP)
--------------------------------------------------------------------------------
Acoustic measure of pronunciation quality from ML model.

Formula: GOP = (1/T) * Sum(log P(p_t | x_t))

Where:
- T = Number of frames
- P(p_t | x_t) = Probability of phoneme p at frame t

Interpretation:
- GOP ≈ 0: Excellent pronunciation (high confidence)
- GOP ≈ -1: Good pronunciation
- GOP ≈ -2: Fair pronunciation
- GOP < -3: Poor pronunciation

Advantage: Works on audio directly, not dependent on transcription accuracy.


LONGEST FLUENT RUN (LFR)
--------------------------------------------------------------------------------
Maximum consecutive words without dysfluency.

Calculation:
1. Mark each word as fluent or dysfluent
2. Find longest streak of fluent words
3. LFR with tolerance allows 1-2 minor hesitations

Clinical significance:
- Low LFR (<3): Severe fluency impairment
- Medium LFR (3-6): Moderate impairment
- High LFR (>6): Mild or no impairment


SSI-4 SCORE (Stuttering Severity Instrument)
--------------------------------------------------------------------------------
Standardized stuttering severity measure.

Components:
1. Frequency Score (% stuttered syllables)
   - <1%: 2 pts, 1-2%: 4 pts, 2-3%: 5 pts, etc.

2. Duration Score (average stutter length)
   - <0.5s: 2 pts, 0.5-1s: 4 pts, 1-2s: 6 pts, etc.

Total Score interpretation:
- 0-10: Normal fluency
- 11-20: Mild stuttering
- 21-30: Moderate stuttering
- 31-40: Severe stuttering
- 41+: Very severe stuttering


================================================================================
                    CLINICAL USE CASES
================================================================================

1. INITIAL ASSESSMENT
--------------------------------------------------------------------------------
- Patient completes session with multiple prompts
- System establishes baseline metrics
- Identifies primary error patterns
- Generates recommendations for therapy focus

Example output:
"Patient shows consistent final consonant deletion (PER=0.35).
 Phonological system partially preserved (WPER=0.18).
 Recommend targeting velar stops /k/, /g/."


2. PROGRESS MONITORING
--------------------------------------------------------------------------------
- Compare metrics across sessions
- Track improvement in specific phonemes
- Visualize trends on dashboard
- Generate progress reports

Example:
Session 1: PER=0.40, LFR=3
Session 5: PER=0.25, LFR=5
→ 37% improvement in pronunciation
→ 67% improvement in fluency


3. PARAPHASIA ANALYSIS
--------------------------------------------------------------------------------
- Differentiates phonological vs semantic errors
- Tracks word-finding patterns
- Identifies circumlocution strategies

Example patterns:
- Phonological paraphasia: "spoon" → "spood" (sound-based)
- Semantic paraphasia: "spoon" → "fork" (category-based)
- Circumlocution: "the thing you eat soup with"


4. FLUENCY DISORDER ASSESSMENT
--------------------------------------------------------------------------------
- Quantifies stuttering severity
- Identifies dysfluency types
- Tracks response to treatment

Example report:
"SSI-4 Score: 24 (Moderate severity)
 Primary dysfluency: Sound repetitions (60%)
 Secondary behaviors: Blocks (25%)
 LFR improved from 2 to 4 words over 3 sessions"


================================================================================
                    SUCCESSFULLY IMPLEMENTED FEATURES
================================================================================

1. AUTOMATIC SPEECH RECOGNITION (ASR)
   [x] Faster-Whisper model integration (small/medium models)
   [x] CPU-optimized inference (int8 quantization)
   [x] Word-level timestamps with confidence scores
   [x] Multi-format audio support (webm, wav, mp3)
   [x] PyAV-based audio conversion (bundled FFmpeg)
   [x] English-only transcription to prevent language detection errors

2. WORD-LEVEL METRICS
   [x] Word Error Rate (WER) calculation using jiwer
   [x] Speech rate (words per minute)
   [x] Pause ratio (silence percentage)
   [x] Segment-level analysis

3. ML-BASED PHONEME ANALYSIS (Wav2Vec2)
   [x] Wav2Vec2 acoustic phoneme recognition
   [x] Direct audio-to-IPA phoneme mapping
   [x] IPA to ARPAbet conversion for consistent comparison
   [x] Goodness of Pronunciation (GOP) scores per phoneme
   [x] ML-based Phoneme Error Rate (PER_ML)
   [x] Dynamic programming alignment (expected vs detected)
   [x] Model confidence scores
   [x] Frame-level probability analysis
   [x] Database persistence of all ML phoneme metrics

4. RULE-BASED PHONEME ANALYSIS (Enhanced)
   [x] CMU Pronouncing Dictionary integration (134,000+ words)
   [x] Weighted Phoneme Error Rate (WPER) with phonetic distance
   [x] Phonetic feature-based distance calculation
   [x] Error type classification (substitution, deletion, insertion, metathesis)
   [x] Conduite d'approche detection (progressive improvement)
   [x] Multi-attempt segmentation and analysis
   [x] Phoneme class error tracking
   [x] Clinical insights generation based on error patterns

5. ENHANCED SEMANTIC ANALYSIS
   [x] Sentence transformer-based similarity
   [x] Multi-tier classification (correct, partial, wrong, circumlocution)
   [x] Position-based semantic paraphasia detection
   [x] Phonological vs semantic paraphasia distinction
   [x] WordNet-based category similarity
   [x] Object feature matching for circumlocution detection
   [x] Clinical significance notes

6. ML-BASED FLUENCY & STUTTERING ANALYSIS
   [x] Silero VAD for voice activity detection
   [x] ML-based pause detection and classification
   [x] ML-based stuttering detection (repetitions, prolongations, blocks)
   [x] Longest Fluent Run (LFR) with tolerance
   [x] SSI-4 approximation (Stuttering Severity Instrument)
   [x] Speech rate metrics (articulation rate, speaking rate)
   [x] Rate variability analysis

7. DATA PERSISTENCE & SESSION MANAGEMENT
   [x] Automatic database saving of all analysis results
   [x] ML phoneme analysis persistence (GOP, PER_ML, alignment)
   [x] Session tracking (grouping multiple prompts)
   [x] Patient identification and longitudinal tracking
   [x] Comprehensive query functions
   [x] API endpoints for therapist insights

8. DYNAMIC PROMPT GENERATION (LLM-Powered)
   [x] Ollama integration (local, privacy-preserving)
   [x] Default model: gemma2:2b (lightweight, fast)
   [x] Support for ANY object (unlimited vocabulary)
   [x] Database caching for instant reuse
   [x] Graceful fallback to template-based prompts

9. THERAPIST DASHBOARD
   [x] Patient progress visualization
   [x] Interactive Chart.js visualizations
   [x] Clinical insights panel
   [x] Recent sessions table with expandable details
   [x] "View Full Analysis" button linking to analysis page

10. USER INTERFACE
    [x] Browser-based audio recording
    [x] Automatic silence detection (VAD)
    [x] Comprehensive result display
    [x] Session management

11. AR OBJECT DETECTION INTERFACE (NEW!)
    [x] YOLOv8n ONNX model for browser-based object detection
    [x] ONNX Runtime Web for client-side ML inference
    [x] Real-time camera feed with WebRTC
    [x] Bounding box visualization with confidence scores
    [x] Support for 80 COCO object classes
    [x] Integrated prompt generation workflow
    [x] Audio recording with auto-stop on silence
    [x] Full-screen immersive therapy experience

12. DETAILED ANALYSIS PAGE (NEW!)
    [x] Standalone recording analysis viewer
    [x] URL-based recording selection (?recording_id=X)
    [x] Complete breakdown of all analysis metrics
    [x] Phoneme error tables with clinical notes
    [x] ML phoneme analysis with GOP scores
    [x] Fluency analysis with SSI-4 severity
    [x] Linked from dashboard for easy access


================================================================================
                    TECHNICAL SPECIFICATIONS
================================================================================

BACKEND STACK:
  - Framework: FastAPI
  - ASGI Server: Uvicorn
  - Database: SQLite with SQLAlchemy ORM
  - ML Models:
      * Faster-Whisper (small/medium) for ASR
      * Wav2Vec2 (facebook/wav2vec2-lv-60-espeak-cv-ft) for phoneme recognition
      * Silero VAD for voice activity detection
      * Sentence-Transformers (all-MiniLM-L6-v2) for semantics
  - Phoneme Analysis:
      * CMU Pronouncing Dictionary (134,000+ words)
      * phonemizer + espeak-ng for G2P
      * Edit distance algorithms with phonetic weighting

FRONTEND STACK:
  - Pure HTML5 + Vanilla JavaScript (no frameworks)
  - Web APIs:
      * MediaRecorder API (audio capture)
      * Web Audio API (silence detection)
      * Fetch API (HTTP communication)
      * getUserMedia API (camera access for AR)
  - Chart.js for data visualization
  - ONNX Runtime Web for browser-based ML inference
  - YOLOv8n ONNX model (~13MB) for object detection

AUDIO PROCESSING:
  - Supported formats: webm, wav, mp3, ogg, flac
  - PyAV for audio conversion (bundled FFmpeg)
  - Silero VAD for speech segment detection
  - Auto-stop: 900ms silence or 8s timeout

KEY DEPENDENCIES:
  - fastapi, uvicorn
  - sqlalchemy, python-multipart
  - faster-whisper
  - torch, torchaudio
  - transformers (Wav2Vec2)
  - sentence-transformers
  - silero-vad
  - phonemizer
  - numpy, scipy
  - noisereduce
  - jiwer, editdistance
  - av (PyAV for audio conversion)
  - nltk (WordNet, CMUdict)


================================================================================
                    FILE STRUCTURE
================================================================================

FINAL PROJECT/
+-- backend/
|   +-- main.py                      # FastAPI application entry point
|   +-- config.py                    # Configuration (DB_URL, model paths)
|   +-- requirements.txt             # Python dependencies
|   |
|   +-- routers/
|   |   +-- analysis.py              # API endpoints
|   |
|   +-- services/
|   |   +-- speech_processing.py     # Main analysis pipeline orchestrator
|   |   +-- semantic_analysis.py     # Basic semantic evaluation
|   |   +-- enhanced_semantic_analysis.py  # Advanced semantic with paraphasia
|   |   +-- phoneme_analysis.py      # Basic phoneme analysis
|   |   +-- enhanced_phoneme_analysis.py   # WPER, conduite d'approche
|   |   +-- phoneme_features.py      # Phonetic feature distances
|   |   +-- phoneme_mapping.py       # IPA <-> ARPAbet conversion
|   |   +-- phoneme_lookup.py        # CMU Dict wrapper
|   |   +-- ml_phoneme.py            # Wav2Vec2 phoneme recognition
|   |   +-- ml_models.py             # ML model registry/loader
|   |   +-- ml_vad.py                # Silero VAD wrapper
|   |   +-- ml_stutter.py            # ML-based stuttering detection
|   |   +-- fluency_analysis.py      # Basic fluency analysis
|   |   +-- enhanced_fluency_analysis.py   # SSI-4, rate variability
|   |   +-- llm_prompt_generator.py  # Ollama LLM integration
|   |   +-- prompts.py               # Legacy hardcoded prompts
|   |   +-- metrics.py               # WER, rate, pause calculations
|   |   +-- audio.py                 # Audio preprocessing
|   |
|   +-- models/
|   |   +-- whisper_model.py         # Whisper ASR singleton
|   |   +-- cache/                   # Hugging Face model cache
|   |
|   +-- database/
|   |   +-- db.py                    # SQLAlchemy engine
|   |   +-- schema.py                # Table initialization
|   |   +-- schema.sql               # Full SQL schema
|   |   +-- persistence.py           # Save/query functions
|   |
|   +-- data/
|   |   +-- objects.py               # Object definitions
|   |   +-- cmudict-0.7b.txt         # CMU Pronouncing Dictionary
|   |
|   +-- utils/
|   |   +-- file_utils.py            # File management
|   |   +-- audio_converter.py       # PyAV audio conversion
|   |
|   +-- frontend/
|   |   +-- recorder.html            # Patient recording interface
|   |   +-- dashboard.html           # Therapist progress dashboard
|   |   +-- ar.html                  # AR object detection + speech practice (NEW)
|   |   +-- analysis.html            # Detailed recording analysis viewer (NEW)
|   |
|   +-- storage/
|   |   +-- audio/                   # Audio recordings (gitignored)
|   |
|   +-- tests/
|       +-- test_phoneme_analysis.py
|       +-- test_fluency_analysis.py
|       +-- test_persistence.py
|
+-- docker-compose.yml               # Gentle service configuration
+-- OLLAMA_SETUP.md                  # LLM installation guide
+-- TESTING_GUIDE.md                 # Dashboard testing guide
+-- project_documentation.txt        # This file


================================================================================
                    DATABASE SCHEMA
================================================================================

recordings table:
  - id (PRIMARY KEY)
  - session_id, patient_id
  - object_name, prompt_text, expected_answer
  - audio_path, transcript

  # Word-level metrics
  - wer, speech_rate, pause_ratio

  # Rule-based phoneme metrics
  - per, wper, total_phonemes
  - phoneme_errors_json, problematic_phonemes_json
  - phoneme_class_errors_json, error_pattern_analysis_json
  - conduite_d_approche_detected, multi_attempt_analysis_json
  - clinical_notes_json

  # ML Phoneme metrics
  - ml_per                       # ML-based PER
  - ml_gop                       # Overall GOP score
  - ml_confidence                # Model confidence
  - ml_detected_phonemes_json    # Detected ARPAbet sequence
  - ml_detected_ipa_json         # Detected IPA sequence
  - ml_alignment_json            # (expected, detected) pairs
  - ml_phoneme_scores_json       # Per-phoneme GOP scores

  # Semantic metrics
  - semantic_classification, semantic_score
  - direct_similarity, category_similarity
  - circumlocution_detected, circumlocution_features_json
  - semantic_paraphasia_json

  # Enhanced fluency metrics
  - longest_fluent_run, lfr_with_tolerance
  - total_pauses, hesitation_count, block_count
  - articulation_rate, speaking_rate
  - ssi_total_score, ssi_severity
  - stuttering_events_json, pauses_json, fluency_notes_json

  # Metadata
  - analysis_mode, disorder_type, created_at


================================================================================
                    API ENDPOINTS
================================================================================

POST /session/start
  - Initialize new session
  - Parameters: object_name (optional), patient_id (optional)
  - Returns: session_id, first prompt

POST /session/next-prompt
  - Get next prompt in session
  - Parameters: session_id
  - Returns: prompt_text, expected_answer

POST /record
  - Main analysis endpoint
  - Parameters: session_id, object_name, prompt_text, expected_answer, audio
  - Returns: Complete analysis JSON

POST /generate-prompts
  - Generate prompts for new object using LLM
  - Parameters: object_name
  - Returns: List of prompts with expected answers

GET /patient/{patient_id}/history
  - Get recording history for patient
  - Returns: List of recordings with all metrics

GET /patient/{patient_id}/progress
  - Get aggregated progress metrics
  - Returns: Trends, averages, improvements

GET /session/{session_id}/summary
  - Get session-level summary
  - Returns: Aggregated metrics for session

GET /recording/{recording_id}
  - Get single recording with full analysis details
  - Returns: Complete recording data with parsed JSON fields

GET /ollama/status
  - Check if Ollama LLM service is available
  - Returns: Status and available models

GET /generated-objects
  - List all objects with cached prompts
  - Returns: List of object names


================================================================================
                    USAGE INSTRUCTIONS
================================================================================

SETUP:

1. Install Python dependencies:
   cd backend
   pip install -r requirements.txt

2. Install espeak-ng (required for phonemizer):
   Windows: winget install eSpeak-NG.eSpeak-NG
   Linux: apt-get install espeak-ng
   Mac: brew install espeak-ng

3. (Optional) Install Ollama for LLM prompts:
   See OLLAMA_SETUP.md

4. Start the backend server:
   cd backend
   uvicorn main:app --reload --port 8001

5. Open frontend interfaces:
   http://localhost:8001           - Patient Recording Interface
   http://localhost:8001/recorder  - Patient Recording Interface (same)
   http://localhost:8001/ar        - AR Object Detection + Speech Practice (NEW)
   http://localhost:8001/dashboard - Therapist Progress Dashboard
   http://localhost:8001/analysis?recording_id=X - Detailed Analysis (NEW)


================================================================================
                    RESEARCH APPLICATIONS
================================================================================

This system enables research on:

1. Acoustic phoneme analysis in disordered speech
2. GOP-based pronunciation assessment in aphasia
3. Longitudinal tracking with objective ML metrics
4. Comparison of text-based vs acoustic phoneme analysis
5. Semantic paraphasia patterns and recovery
6. Fluency disorder progression with VAD-based metrics
7. SSI-4 validation with automated scoring
8. Multi-modal speech assessment (acoustic + linguistic)
9. AR-enhanced speech therapy interventions (NEW)
10. Edge ML for accessible therapy tools (browser-based object detection)


================================================================================
                    KNOWN LIMITATIONS & FUTURE WORK
================================================================================

CURRENT LIMITATIONS:

1. No user authentication
2. Session storage is in-memory (lost on server restart)
3. SONIVA Whisper model requires manual download
4. espeak-ng required for some phoneme operations
5. English-only support

FUTURE ENHANCEMENTS:

1. Real-time pronunciation feedback
2. Mobile app version
3. Multi-language support
4. HIPAA-compliant deployment
5. Integration with EHR systems
6. SONIVA model integration (aphasia-specific ASR)


================================================================================
                    CITATION
================================================================================

If you use this system in your research, please cite:

[Your Name et al.] (2026). AI-Powered Speech Therapy System with
ML-Based Phoneme Analysis for Aphasia Assessment. [Journal Name].


================================================================================
                    ACKNOWLEDGMENTS
================================================================================

This system uses the following open-source tools:

- Wav2Vec2 (Facebook AI Research)
- Silero VAD (silero-team)
- Faster-Whisper (SYSTRAN)
- Sentence Transformers (UKPLab)
- CMU Pronouncing Dictionary
- phonemizer + espeak-ng
- Ollama (local LLM runtime)
- FastAPI, Chart.js
- YOLOv8 (Ultralytics) - Object Detection
- ONNX Runtime Web - Browser ML Inference


Last Updated: January 17, 2026
Version: 5.0.0
Status: Research MVP with ML-Powered Analysis + AR Object Detection
Features: Wav2Vec2 Phoneme | Silero VAD | Enhanced Semantics | YOLOv8 AR Detection | Analysis Page

================================================================================
                          END OF DOCUMENTATION
================================================================================
