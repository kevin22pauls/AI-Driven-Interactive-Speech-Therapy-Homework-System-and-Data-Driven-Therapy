================================================================================
           AI-POWERED SPEECH THERAPY HOMEWORK SYSTEM
            COMPREHENSIVE PROJECT DOCUMENTATION
================================================================================

PROJECT OVERVIEW
================================================================================
This is a research-grade speech therapy system designed for patients with
aphasia and speech disorders. The system provides automated speech analysis
at multiple levels: word-level, phoneme-level, and semantic-level.

Intended Use: Clinical research and speech therapy assessment
Target Population: Aphasia patients, dysarthria, and speech sound disorders
Research Status: Suitable for journal publication


SYSTEM ARCHITECTURE
================================================================================

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           FRONTEND LAYER                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  recorder.html - Browser-based Recording Interface               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - MediaRecorder API for audio capture                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Real-time silence detection (Web Audio API)                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Result visualization with phoneme error highlighting          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚Üì HTTP POST /record
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         BACKEND API LAYER                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  main.py - FastAPI Application                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  routers/analysis.py - API Endpoints                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - /session/start         - Initialize session                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - /session/next-prompt   - Get next prompt                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - /session/change-object - Switch object                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - /record                - Upload & analyze audio             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        ANALYSIS PIPELINE                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  services/speech_processing.py - Main Pipeline                   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ASR        ‚îÇ  Word-Level     ‚îÇ  Phoneme-Level    ‚îÇ  Semantic    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (Whisper)  ‚îÇ  Metrics        ‚îÇ  Analysis         ‚îÇ  Evaluation  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ        ‚Üì              ‚Üì                   ‚Üì                  ‚Üì          ‚îÇ
‚îÇ  Transcription   WER, Rate,         PER, Error          Similarity,    ‚îÇ
‚îÇ  + Timestamps    Pause Ratio        Classification      Classification ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        DATA PERSISTENCE                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  SQLite Database (speechtherapy.db)                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - recordings table: All analysis results                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - patient_phoneme_history: Longitudinal phoneme tracking        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


SUCCESSFULLY IMPLEMENTED FEATURES
================================================================================

1. AUTOMATIC SPEECH RECOGNITION (ASR)
   ‚úì Faster-Whisper model integration (small/tiny models)
   ‚úì CPU-optimized inference (int8 quantization)
   ‚úì Segment-level timestamps
   ‚úì Multi-format audio support (webm, wav, mp3)
   ‚úì Robust error handling for transcription failures

2. WORD-LEVEL METRICS
   ‚úì Word Error Rate (WER) calculation using jiwer
   ‚úì Speech rate (words per minute)
   ‚úì Pause ratio (silence percentage)
   ‚úì Segment-level analysis

3. PHONEME-LEVEL ANALYSIS ‚≠ê NEW
   ‚úì CMU Pronouncing Dictionary integration (134,000+ words)
   ‚úì Phoneme Error Rate (PER) calculation
   ‚úì Error type classification:
      - Substitutions (e.g., AA ‚Üí UW in "bottle" ‚Üí "bootle")
      - Deletions (missing phonemes)
      - Insertions (extra phonemes)
   ‚úì Problematic phoneme identification
   ‚úì Clinical insights generation
   ‚úì Forced alignment support (Gentle integration)
   ‚úì ARPAbet to IPA conversion for display
   ‚úì Database storage for longitudinal tracking

4. SEMANTIC ANALYSIS
   ‚úì Sentence transformer-based similarity (all-MiniLM-L6-v2)
   ‚úì Three-tier classification (correct/partial/wrong)
   ‚úì Object identification verification
   ‚úì Paraphrase detection
   ‚úì Semantic substitution tracking
   ‚úì Clinical marker identification

5. FLUENCY & STUTTERING ANALYSIS ‚≠ê NEW
   ‚úì Longest Fluent Run (LFR) calculation - key progress metric
   ‚úì Pause detection and classification:
      - Hesitations (300ms - 1s pauses)
      - Blocks (>1s pauses)
      - Pause location tracking (beginning, middle, end)
   ‚úì Stuttering event detection:
      - Word repetitions (e.g., "I I I want")
      - Prolongations (stretched sounds based on duration)
      - Interjections (um, uh, like, etc.)
   ‚úì Fluency metrics:
      - Dysfluencies per 100 words
      - Dysfluencies per minute
      - Fluency percentage
      - Speech rate variability (consistency of pace)
   ‚úì Clinical insights for fluency therapy
   ‚úì Automated clinical recommendations

6. DATA PERSISTENCE & SESSION MANAGEMENT ‚≠ê‚≠ê‚≠ê NEW
   ‚úì Automatic database saving of all analysis results
   ‚úì Session tracking (grouping multiple prompts together)
   ‚úì Patient identification and longitudinal tracking
   ‚úì Comprehensive query functions:
      - Patient recording history with filtering
      - Progress tracking over time (trends)
      - Phoneme trend analysis (specific phoneme improvement)
      - Session summaries with aggregate metrics
   ‚úì Patient phoneme history tracking table
   ‚úì Aggregated progress metrics:
      - Average fluency, PER, WER, LFR
      - LFR trend over time (daily averages)
      - Most problematic phonemes ranked
      - Performance by object
   ‚úì API endpoints for therapist insights
   ‚úì Graceful error handling (saves don't block analysis)

7. DYNAMIC PROMPT GENERATION SYSTEM ‚≠ê‚≠ê‚≠ê‚≠ê NEW
   ‚úì LLM-powered prompt generation for unlimited objects
   ‚úì Ollama integration (zero-cost, local, privacy-preserving)
   ‚úì Support for ANY object (e.g., toothbrush, umbrella, laptop, spoon)
   ‚úì Contextually appropriate question generation:
      - Identification questions ("What is this object?")
      - Functional questions ("What do you use it for?")
      - Descriptive questions ("What color is it?")
      - Contextual questions ("When do you use it?")
      - Repetition sentences (simple and advanced)
   ‚úì Multiple expected answers per question for semantic evaluation
   ‚úì Database caching for instant reuse (generated_prompts table)
   ‚úì Graceful fallback to template-based prompts if LLM unavailable
   ‚úì Real-time LLM status indicators in UI (üü¢ Ready / üü° Fallback)
   ‚úì Prompt source tracking (ü§ñ LLM Generated / üíæ Cached / üìù Template)
   ‚úì Smart prompt rotation (no duplicates until all used)
   ‚úì Session-based prompt management
   ‚úì Compatible with legacy hardcoded objects (bottle, cup, phone, book, chair)
   ‚úì English-only transcription to prevent language detection errors

8. THERAPIST DASHBOARD ‚≠ê‚≠ê‚≠ê NEW
   ‚úì Patient progress visualization dashboard
   ‚úì Interactive Chart.js visualizations:
      - LFR trend over time (line chart)
      - PER trend over time (line chart)
      - Fluency percentage trend (line chart)
      - Most problematic phonemes (bar chart, top 10)
   ‚úì Clinical insights panel with automated recommendations
   ‚úì Summary metric cards (total recordings, avg PER, avg LFR, avg fluency)
   ‚úì Recent sessions table with expandable details
   ‚úì Session drill-down showing:
      - Full transcript and all metrics
      - Detailed phoneme errors list
      - Stuttering events breakdown
      - Clinical notes per recording
   ‚úì Patient selector with real-time data loading
   ‚úì Refresh capability for updated data
   ‚úì Responsive design for mobile/tablet/desktop
   ‚úì Color-coded insights (positive, neutral, focus areas)

9. USER INTERFACE (Patient/Recorder)
   ‚úì Browser-based audio recording
   ‚úì Automatic silence detection (VAD)
   ‚úì Real-time status updates
   ‚úì Comprehensive result display:
      - Transcript
      - Semantic evaluation with color coding
      - Phoneme error table with highlighting
      - Fluency analysis with LFR and stuttering events ‚≠ê NEW
      - Speech metrics
      - Clinical insights
   ‚úì Collapsible detailed views for:
      - Phoneme errors
      - Stuttering events ‚≠ê NEW
      - Pause details ‚≠ê NEW
   ‚úì Session management buttons

10. DATABASE & STORAGE
   ‚úì SQLite database with comprehensive schema
   ‚úì Recordings table with all metrics
   ‚úì Patient phoneme history tracking
   ‚úì JSON storage for complex data
   ‚úì Indexed for performance
   ‚úì Audio file management with timestamped names

9. TESTING & QUALITY ASSURANCE
   ‚úì Unit tests for phoneme analysis
   ‚úì Test coverage for PER calculation
   ‚úì Alignment algorithm testing
   ‚úì Clinical note generation tests
   ‚úì CMU dictionary lookup tests
   ‚úì Fluency analysis tests ‚≠ê NEW:
      - Pause detection testing
      - Repetition detection testing
      - LFR calculation testing
      - Interjection detection testing
      - Speech rate variability testing

10. DEPLOYMENT & INFRASTRUCTURE
   ‚úì Docker Compose configuration for Gentle
   ‚úì Modular service architecture
   ‚úì Environment variable configuration
   ‚úì Graceful degradation (works without Gentle)
   ‚úì Comprehensive logging


TECHNICAL SPECIFICATIONS
================================================================================

BACKEND STACK:
  - Framework: FastAPI 0.x
  - ASGI Server: Uvicorn
  - Database: SQLite with SQLAlchemy ORM
  - ML Models:
      * Faster-Whisper (small/tiny) for ASR
      * Sentence-Transformers (all-MiniLM-L6-v2) for semantics
  - Phoneme Analysis:
      * CMU Pronouncing Dictionary (134,000+ words)
      * Edit distance algorithms for alignment
      * Gentle forced aligner (optional, via Docker)

FRONTEND STACK:
  - Pure HTML5 + Vanilla JavaScript (no frameworks)
  - Web APIs:
      * MediaRecorder API (audio capture)
      * Web Audio API (silence detection)
      * Fetch API (HTTP communication)

AUDIO PROCESSING:
  - Supported formats: webm, wav, mp3, ogg, flac
  - Silence detection: RMS-based energy threshold
  - Auto-stop: 900ms silence or 8s timeout

DEPENDENCIES (requirements.txt):
  - fastapi
  - uvicorn
  - sqlalchemy
  - python-multipart
  - pydantic
  - faster-whisper
  - torch
  - numpy
  - scipy
  - noisereduce
  - silero-vad
  - jiwer
  - sentence-transformers
  - soundfile
  - editdistance
  - requests
  - scikit-learn


FILE STRUCTURE
================================================================================

FINAL PROJECT/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                      # FastAPI application entry point
‚îÇ   ‚îú‚îÄ‚îÄ config.py                    # Configuration settings
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analysis.py              # API endpoints
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ speech_processing.py     # Main analysis pipeline (+ English-only Whisper)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ semantic_analysis.py     # Semantic evaluation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ phoneme_analysis.py      # ‚≠ê Phoneme-level analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ phoneme_lookup.py        # ‚≠ê CMU Dict wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ forced_alignment.py      # ‚≠ê Gentle integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fluency_analysis.py      # ‚≠ê‚≠ê Fluency & stuttering detection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_prompt_generator.py  # ‚≠ê‚≠ê‚≠ê‚≠ê NEW: Ollama LLM integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts.py               # Legacy hardcoded prompts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py               # WER, rate, pause calculations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ audio.py                 # Audio preprocessing
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ whisper_model.py         # Whisper ASR singleton
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db.py                    # SQLAlchemy engine
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema.py                # Table initialization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema.sql               # ‚≠ê SQL schema (fluency + generated_prompts table)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ persistence.py           # ‚≠ê‚≠ê‚≠ê Save/query + prompt caching functions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ migrate_add_fluency_columns.py  # Migration script for old databases
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ objects.py               # Object definitions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cmudict-0.7b.txt         # ‚≠ê CMU Pronouncing Dictionary
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ file_utils.py            # File management
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recorder.html            # ‚≠ê‚≠ê‚≠ê‚≠ê Patient interface with dynamic objects
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dashboard.html           # ‚≠ê‚≠ê‚≠ê Therapist progress dashboard
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ audio/                   # Audio recordings (gitignored)
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ tests/                       # ‚≠ê NEW
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ test_phoneme_analysis.py # Phoneme unit tests
‚îÇ       ‚îú‚îÄ‚îÄ test_fluency_analysis.py # ‚≠ê‚≠ê Fluency unit tests
‚îÇ       ‚îî‚îÄ‚îÄ test_persistence.py      # ‚≠ê‚≠ê‚≠ê Database persistence tests
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml               # ‚≠ê Gentle service configuration
‚îú‚îÄ‚îÄ OLLAMA_SETUP.md                  # ‚≠ê‚≠ê‚≠ê‚≠ê NEW: LLM installation & usage guide
‚îú‚îÄ‚îÄ TESTING_GUIDE.md                 # ‚≠ê‚≠ê‚≠ê Dashboard testing guide
‚îú‚îÄ‚îÄ .gitignore                       # Git ignore rules
‚îî‚îÄ‚îÄ PROJECT_DOCUMENTATION.txt        # ‚≠ê This file


KEY ALGORITHMS
================================================================================

1. PHONEME ERROR RATE (PER) CALCULATION:

   PER = (S + D + I) / N

   Where:
   - S = Number of phoneme substitutions
   - D = Number of phoneme deletions
   - I = Number of phoneme insertions
   - N = Total number of phonemes in reference

   Implementation: Levenshtein distance on phoneme sequences

2. PHONEME ALIGNMENT:

   Uses dynamic programming (similar to sequence alignment in bioinformatics)
   to find optimal alignment between expected and actual phoneme sequences.

   Algorithm: Wagner-Fischer algorithm (edit distance with backtracking)

   Example:
   Expected: B  AH  T  AH  L    (bottle)
   Actual:   B  UW  T  AH  L    (bootle)
   Error:       ^                (substitution: AH ‚Üí UW)

3. SEMANTIC SIMILARITY:

   Uses sentence transformers to compute cosine similarity between
   expected answer and transcript embeddings.

   Thresholds:
   - > 0.85: Correct
   - 0.60 - 0.85: Partial
   - < 0.60: Wrong

4. SPEECH RATE CALCULATION:

   Speech Rate = Total Words / (End Time - Start Time) * 60

   Units: Words per minute (WPM)

5. PAUSE RATIO:

   Pause Ratio = Total Pause Duration / Total Speech Duration

   Calculated from inter-segment gaps

6. LONGEST FLUENT RUN (LFR): ‚≠ê‚≠ê NEW

   LFR = Maximum consecutive words spoken without dysfluencies

   Dysfluencies include:
   - Pauses > 300ms (hesitations)
   - Pauses > 1000ms (blocks)
   - Word repetitions
   - Interjections (filler words)
   - Prolongations

   Algorithm:
   1. Mark all dysfluent positions in word sequence
   2. Find longest continuous run of non-dysfluent words
   3. Return count of words in longest run

   Example:
   "I... I want um to drink water" ‚Üí LFR = 3 ("to drink water")

7. PAUSE DETECTION: ‚≠ê‚≠ê NEW

   Pause Duration = word[i+1].start - word[i].end

   Classification:
   - < 300ms: Normal (not counted)
   - 300ms - 1000ms: Hesitation
   - > 1000ms: Block/significant pause

   Location tracking:
   - Beginning: Pause after first word
   - Middle: Pause in middle of utterance
   - End: Pause before last word

8. STUTTERING EVENT DETECTION: ‚≠ê‚≠ê NEW

   Repetitions:
   - Detect consecutive identical words
   - Count repetition frequency
   - Example: "I I I" ‚Üí 3 repetitions

   Interjections:
   - Match against filler word list (um, uh, like, etc.)
   - Flag as dysfluency markers

   Prolongations:
   - Calculate expected word duration: avg_char_duration * word_length
   - Detect outliers > 2 standard deviations above mean
   - Flag words with duration > 500ms as potentially prolonged

9. FLUENCY PERCENTAGE: ‚≠ê‚≠ê NEW

   Fluency % = 100 - (Dysfluencies / Total Words * 100)

   Where dysfluencies = pauses + stuttering events

10. SPEECH RATE VARIABILITY: ‚≠ê‚≠ê NEW

    Measures consistency of speaking pace (coefficient of variation)

    Algorithm:
    1. Calculate speech rate for 3-word sliding windows
    2. Compute mean and standard deviation of rates
    3. CV = std_dev / mean

    Higher CV indicates more variable/inconsistent pace (common in dysfluency)


CLINICAL INSIGHTS GENERATION
================================================================================

The system automatically generates clinical recommendations based on:

1. Overall PER score
2. Error type distribution
3. Specific problematic phonemes
4. Error patterns (e.g., consistent substitutions)
5. Phoneme class (vowels, fricatives, liquids)

Example Output:
"Moderate phoneme errors detected"
"Most problematic phonemes: AA (2x), AH (1x)"
"Common substitution pattern: AA‚ÜíUW"
"Consider vowel discrimination exercises"


RESEARCH APPLICATIONS
================================================================================

This system enables research on:

1. Phoneme-level error patterns in aphasia
2. Longitudinal tracking of pronunciation improvement ‚≠ê‚≠ê‚≠ê ENABLED
3. Identification of problematic phonemes per patient ‚≠ê‚≠ê‚≠ê ENABLED
4. Correlation between semantic and phonetic errors
5. Automated homework compliance monitoring ‚≠ê‚≠ê‚≠ê ENABLED (session tracking)
6. Large-scale speech therapy outcome studies ‚≠ê‚≠ê‚≠ê ENABLED (database persistence)
7. Fluency disorder progression tracking ‚≠ê‚≠ê‚≠ê ENABLED
8. Stuttering pattern analysis and treatment efficacy ‚≠ê‚≠ê‚≠ê ENABLED
9. Longest Fluent Run (LFR) as progress indicator ‚≠ê‚≠ê‚≠ê ENABLED
10. Objective dysfluency frequency measurement ‚≠ê‚≠ê‚≠ê ENABLED
11. Patient-specific phoneme trend analysis ‚≠ê‚≠ê‚≠ê NEW
12. Multi-session progress comparison ‚≠ê‚≠ê‚≠ê NEW
13. Object-specific performance tracking ‚≠ê‚≠ê‚≠ê NEW


USAGE INSTRUCTIONS
================================================================================

SETUP:

1. Install Python dependencies:
   cd backend
   pip install -r requirements.txt

2. (Optional) Start Gentle service for forced alignment:
   docker-compose up -d gentle

3. Start the backend server:
   cd backend
   uvicorn main:app --reload --port 8000

4. Open frontend:
   - Patient Interface: Open backend/frontend/recorder.html in a web browser
   - Therapist Dashboard: Open backend/frontend/dashboard.html in a web browser

PATIENT WORKFLOW (recorder.html):

1. Click "Get Prompt" to start a session
2. Read the prompt aloud (e.g., "What is this object?")
3. Click "Start Recording" and speak your answer
4. Recording stops automatically after silence or 8 seconds
5. View comprehensive analysis:
   - Transcript
   - Semantic evaluation (correct/partial/wrong)
   - Phoneme Error Rate (PER)
   - Problematic phonemes
   - Clinical insights
   - Detailed error table

6. Click "Get Prompt" again for another prompt with same object
7. Click "New Object" to switch to a different object

THERAPIST WORKFLOW (dashboard.html): ‚≠ê‚≠ê‚≠ê NEW

1. Enter patient ID (e.g., "john_doe", "patient123")
2. Click "Load Patient Data" to fetch progress metrics
3. View Summary Dashboard:
   - Metric Cards: Total recordings, avg PER, avg LFR, avg fluency
   - Clinical Insights: Automated recommendations based on progress
   - Charts: LFR trend, PER trend, fluency trend, problematic phonemes
4. Analyze Progress Trends:
   - Check if LFR is increasing over time (improvement in fluency)
   - Check if PER is decreasing over time (improvement in pronunciation)
   - Identify which phonemes need focused practice
5. Review Recent Sessions:
   - Scroll to sessions table at bottom
   - Click any session row to expand details
   - View full transcript, metrics, phoneme errors, stuttering events
6. Identify Focus Areas:
   - Read clinical insights for automated recommendations
   - Check "Most Problematic Phonemes" chart
   - Plan next therapy session based on data
7. Click "Refresh" to update with latest data

Dashboard Features:
- Real-time data loading from API
- Interactive charts with tooltips (hover to see details)
- Color-coded insights (green = positive, yellow = neutral, red = focus)
- Expandable session details for deep-dive analysis
- Responsive design works on desktop, tablet, mobile


API ENDPOINTS
================================================================================

POST /session/start
  - Starts a new session
  - Returns: session_id, object, prompt_text, expected_answer

POST /session/next-prompt
  - Gets next prompt for current object
  - Params: session_id
  - Returns: prompt_text, expected_answer

POST /session/change-object
  - Changes to a different object
  - Params: session_id, new_object (optional)
  - Returns: new prompt for new object

POST /record
  - Uploads audio and performs analysis
  - Params: session_id, object_name, prompt_text, expected_answer, audio (file)
  - Returns: Complete analysis JSON:
      * transcript
      * wer, speech_rate, pause_ratio
      * semantic_evaluation
      * phoneme_analysis
          - per, total_phonemes
          - errors (detailed list)
          - problematic_phonemes
          - clinical_notes
      * fluency_analysis
          - longest_fluent_run
          - fluency_percentage
          - stuttering_events
          - pauses
          - clinical_notes
      * recording_id (database ID) ‚≠ê‚≠ê‚≠ê NEW

GET /patient/{patient_id}/history  ‚≠ê‚≠ê‚≠ê NEW
  - Get recording history for a patient
  - Params: patient_id, limit (optional), object_name (optional filter)
  - Returns: List of all recordings with full analysis results

GET /patient/{patient_id}/progress  ‚≠ê‚≠ê‚≠ê NEW
  - Get aggregated progress metrics for a patient
  - Returns:
      * total_recordings
      * avg_fluency, avg_per, avg_wer, avg_lfr
      * lfr_trend (daily averages over time)
      * most_problematic_phonemes (ranked by error rate)
      * object_performance (performance by object)

GET /patient/{patient_id}/phoneme/{phoneme}/trend  ‚≠ê‚≠ê‚≠ê NEW
  - Track specific phoneme errors over time
  - Params: patient_id, phoneme (e.g., 'AA', 'TH')
  - Returns: List of recordings where phoneme had errors, with dates

GET /session/{session_id}/summary  ‚≠ê‚≠ê‚≠ê NEW
  - Get summary of all recordings in a session
  - Returns:
      * session_id
      * total_prompts
      * object_name
      * recordings (all recordings in session)
      * session_averages (avg fluency, PER, LFR)

POST /generate-prompts  ‚≠ê‚≠ê‚≠ê‚≠ê NEW
  - Generate speech therapy prompts for any object using LLM
  - Params: object_name (str), force_regenerate (bool, optional)
  - Returns:
      * object: object name
      * questions: array of question objects with expected_answers
      * sentences: array of sentence objects with difficulty levels
      * source: "cache" | "llm" | "fallback"
      * model: LLM model name (if LLM was used)
  - Automatically caches results in database for instant reuse
  - Falls back to template prompts if Ollama unavailable

GET /ollama/status  ‚≠ê‚≠ê‚≠ê‚≠ê NEW
  - Check if Ollama LLM service is running
  - Returns:
      * available: true/false
      * message: status description

GET /generated-objects  ‚≠ê‚≠ê‚≠ê‚≠ê NEW
  - Get list of all objects with cached prompts
  - Returns:
      * objects: array of object names
      * count: total count


DATABASE SCHEMA
================================================================================

recordings table:
  - id (PRIMARY KEY)
  - session_id                              # Groups prompts together
  - object_name
  - prompt_text
  - expected_answer
  - audio_path
  - transcript

  # Word-level metrics
  - wer, speech_rate, pause_ratio

  # Phoneme-level metrics
  - per, total_phonemes
  - phoneme_errors_json                     # JSON array of detailed errors
  - problematic_phonemes_json               # JSON object: {phoneme: count}
  - clinical_notes_json                     # JSON array of insights

  # Semantic metrics
  - semantic_classification                 # correct/partial/wrong
  - semantic_score                          # 0.0 - 1.0

  # Fluency metrics ‚≠ê‚≠ê‚≠ê NEW
  - longest_fluent_run                      # Max consecutive fluent words
  - total_pauses                            # Total pause count
  - hesitation_count                        # 300ms-1s pauses
  - block_count                             # >1s pauses
  - fluency_percentage                      # 0-100%
  - dysfluencies_per_100_words
  - dysfluencies_per_minute
  - speech_rate_variability                 # Coefficient of variation
  - stuttering_events_json                  # JSON array of stuttering events
  - pauses_json                             # JSON array of pause details
  - fluency_notes_json                      # JSON array of clinical notes

  - created_at (TIMESTAMP)

patient_phoneme_history table:              # ‚≠ê‚≠ê‚≠ê Tracks phoneme errors over time
  - id (PRIMARY KEY)
  - patient_id
  - phoneme                                 # e.g., 'AA', 'TH', 'R'
  - error_count                             # Total errors for this phoneme
  - occurrence_count                        # Total times phoneme appeared
  - last_error_date
  - created_at

generated_prompts table:                    # ‚≠ê‚≠ê‚≠ê‚≠ê NEW - Caches LLM-generated prompts
  - id (PRIMARY KEY)
  - object_name (UNIQUE)                    # e.g., 'toothbrush', 'umbrella'
  - prompts_json (TEXT)                     # JSON with questions and sentences
  - model_name (TEXT)                       # e.g., 'llama3.2:3b' or NULL for fallback
  - generation_method (TEXT)                # 'llm' or 'fallback'
  - created_at (TIMESTAMP)
  - last_used_at (TIMESTAMP)                # LRU tracking


CHALLENGES FACED
================================================================================

1. DATABASE SCHEMA MIGRATION:
   - Issue: Old database from previous version lacked fluency columns
   - Impact: Runtime errors when trying to save recordings with new metrics
   - Solution: Created migration script (migrate_add_fluency_columns.py) and
     documented two approaches: (1) Delete old database, (2) Run migration
   - Lesson: SQLite's CREATE TABLE IF NOT EXISTS doesn't alter existing tables

2. SESSION MANAGEMENT SYNCHRONIZATION:
   - Issue: Client-side was creating custom session IDs with patient prefix
     and overriding server's UUID-based session IDs
   - Impact: Server couldn't find sessions in memory, causing 404 errors on
     /session/next-prompt and /session/change-object endpoints
   - Root Cause: Client override at recorder.html:126 (json.session_id = sessionId)
     meant server never stored sessions under the IDs client was requesting
   - Solution: Modified /session/start endpoint to accept patient_id parameter
     and generate session ID with patient prefix server-side. Removed client
     override so both client and server use same session ID.
   - Lesson: Session management must be centralized on server to maintain state

3. JAVASCRIPT EVENT HANDLER BUG:
   - Issue: Dashboard load button passed event object instead of patient ID
   - Impact: API received URL-encoded "[object PointerEvent]" as patient_id
   - Root Cause: loadButton.addEventListener('click', loadPatientData) passed
     event object as first parameter to function expecting patient ID
   - Solution: Changed to arrow function: () => loadPatientData() to call
     function without parameters (extracts patient ID from input field)
   - Lesson: Direct function references in event listeners receive event object

4. WHISPER MULTILINGUAL TRANSCRIPTION:
   - Issue: Whisper transcribing background noise as non-English (Japanese/Chinese)
   - Impact: Gibberish transcripts like "Á¥†„Å£„ÄÅÁ¥†„Å£‚Ä¶„Éï„Ç©„Éº" appearing in results
   - Root Cause: Whisper's multilingual model trying to interpret unclear audio
     as speech from other languages without language constraints
   - Solution: Added language="en" parameter to force English-only transcription,
     enabled VAD filtering (vad_filter=True) with 500ms silence threshold to
     better filter out background noise and silence
   - Lesson: Always constrain language model when target language is known


KNOWN LIMITATIONS & FUTURE WORK
================================================================================

CURRENT LIMITATIONS:

1. No user authentication or patient management
2. Session storage is in-memory (cleared on server restart)
3. Gentle forced alignment is optional (works without it)
4. CMU Dict doesn't include all words (handles gracefully)
5. No GOP (Goodness of Pronunciation) scores yet

FUTURE ENHANCEMENTS:

1. Implement database saving in routers/analysis.py
2. Add patient account management
3. Create therapist dashboard for progress tracking
4. Implement GOP scoring for pronunciation quality
5. Add more sophisticated VAD (Voice Activity Detection)
6. Support for additional languages
7. Real-time feedback during recording
8. Mobile app version
9. IRB compliance features (consent, anonymization)
10. Export to HIPAA-compliant formats


VALIDATION & TESTING
================================================================================

UNIT TESTS:
‚úì Phoneme lookup (CMU Dict)
‚úì PER calculation
‚úì Phoneme alignment algorithm
‚úì Error type classification
‚úì Word extraction from segments
‚úì Pause detection (hesitation, block)
‚úì Stuttering detection (repetitions, interjections)
‚úì LFR calculation with dysfluencies
‚úì Database persistence (save_recording) ‚≠ê‚≠ê‚≠ê NEW
‚úì Patient history queries ‚≠ê‚≠ê‚≠ê NEW
‚úì Progress tracking aggregations ‚≠ê‚≠ê‚≠ê NEW
‚úì Session summaries ‚≠ê‚≠ê‚≠ê NEW
‚úì Phoneme trend tracking ‚≠ê‚≠ê‚≠ê NEW

INTEGRATION TESTING:
(To be implemented)
- End-to-end workflow testing
- API endpoint testing with real audio

CLINICAL VALIDATION:
(Future work for publication)
- Inter-rater reliability with SLPs
- Comparison with gold-standard assessments
- Validation on aphasia patient data


ETHICAL CONSIDERATIONS
================================================================================

1. DATA PRIVACY:
   - Audio files stored locally
   - No cloud transmission by default
   - Database not encrypted (implement for production)

2. CLINICAL USE:
   - System is research-grade, not FDA-approved
   - Should supplement, not replace, SLP assessment
   - Requires informed consent for research use

3. IRB APPROVAL:
   - Required before patient data collection
   - Include data management plan
   - Obtain consent for audio recording

4. BIAS & FAIRNESS:
   - CMU Dict is primarily American English
   - May not accurately represent all dialects
   - Whisper model trained on diverse data


CITATION
================================================================================

If you use this system in your research, please cite:

[Your Name et al.] (2025). AI-Powered Speech Therapy Homework System with
Phoneme-Level Analysis for Aphasia Assessment. [Journal Name]. [DOI]


ACKNOWLEDGMENTS
================================================================================

This system uses the following open-source tools and datasets:

- CMU Pronouncing Dictionary (CMU Sphinx)
- Faster-Whisper (OpenAI Whisper optimized)
- Ollama (local LLM runtime)
- Llama 3.2 (Meta AI)
- Gentle Forced Aligner (lowerquality/gentle)
- Sentence Transformers (UKPLab)
- FastAPI framework
- Chart.js (data visualization)


CONTACT & SUPPORT
================================================================================

For questions, bug reports, or collaboration inquiries:
- GitHub Issues: [repository URL]
- Email: [your email]

Last Updated: January 2, 2026
Version: 3.0.0
Status: Research MVP with Unlimited Object Support
Features: Phoneme + Fluency Analysis | Database Persistence | LLM-Powered Prompts


================================================================================
                          END OF DOCUMENTATION
================================================================================
